{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time \n",
    "from pyDOE import lhs\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot \n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMEGA = 1\n",
    "THETA_0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 1000\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 1000\n",
    "# DeepNN topology (1-sized input [t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [1, 20, 200, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 500\n",
    "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                        epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(N_u, N_f, noise=0.1):\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "    npz = np.load('pendulum_alexis/data/single_random_pendulum_data_L100.npz', allow_pickle=True)\n",
    "    # 50000 states of the pendulum\n",
    "    states_data = npz['states']\n",
    "    rewards_data = npz['rewards']\n",
    "    # 50000 corresponding times for each state of the pendulum\n",
    "    time_data = npz['time']\n",
    "    theta = states_data[:,1]\n",
    "    \n",
    "    theta = theta[:N_f]\n",
    "    #theta = [np.arcsin(x) for x in theta]\n",
    "    #theta = np.array(theta)\n",
    "    time = time_data[:N_f]\n",
    "    \n",
    "    #theta = [np.array(x) for x in theta]\n",
    "    #time = [np.array(y) for y in time]\n",
    "    \n",
    "    idx = np.random.choice(time.shape[0], N_u, replace=False)\n",
    "    \n",
    "    lb = 0\n",
    "    ub = 4*np.pi\n",
    "    # Generating the t points for f with a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the domain\n",
    "    X_f = time #lb + (ub - lb) * lhs(1, N_f) #\n",
    "    Exact_u = theta #THETA_0*np.cos(OMEGA*X_f) #\n",
    "    X_u_train = time #[idx] #lb + (ub - lb) * lhs(1, N_f) #\n",
    "    u_train = theta #[idx] #+ noise*np.random.randn(N_f) # [idx] #THETA_0*np.cos(OMEGA*X_u_train) + noise*np.random.randn(N_f, 1) #\n",
    "    \n",
    "    return X_f, Exact_u, X_u_train, u_train, lb, ub\n",
    "X_f, Exact_u, X_u_train, u_train, lb, ub = prep_data(N_u, N_f)\n",
    "print('lb', lb)\n",
    "print('ub', ub)\n",
    "print('X_f', type(X_f[0]))\n",
    "print('X_u_train', X_u_train[:5])\n",
    "print('u_train', u_train[:5])\n",
    "#print(Exact_u)\n",
    "#print(X_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, frequency=10):\n",
    "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __get_elapsed(self):\n",
    "        return datetime.fromtimestamp(time.time() -\n",
    "                                      self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "    def __get_error_u(self):\n",
    "        return self.error_fn()\n",
    "\n",
    "    def set_error_fn(self, error_fn):\n",
    "        self.error_fn = error_fn\n",
    "\n",
    "    def log_train_start(self, model):\n",
    "        print(\"\\nTraining started\")\n",
    "        print(\"================\")\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "        if epoch % self.frequency == 0:\n",
    "            print(\n",
    "                f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \"\n",
    "                + custom)\n",
    "\n",
    "    def log_train_opt(self, name):\n",
    "        # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "        print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "    def log_train_end(self, epoch, custom=\"\"):\n",
    "        print(\"==================\")\n",
    "        print(\n",
    "            f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \"\n",
    "            + custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"custom_lbfgs.py\"\"\"\n",
    "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_func(s):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = None\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "    \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "  \"\"\"\n",
    "\n",
    "    if config.maxIter == 0:\n",
    "        return\n",
    "\n",
    "    global final_loss, times\n",
    "\n",
    "    maxIter = config.maxIter\n",
    "    maxEval = config.maxEval or maxIter * 1.25\n",
    "    tolFun = config.tolFun or 1e-5\n",
    "    tolX = config.tolX or 1e-19\n",
    "    nCorrection = config.nCorrection or 100\n",
    "    lineSearch = config.lineSearch\n",
    "    lineSearchOpts = config.lineSearchOptions\n",
    "    learningRate = config.learningRate or 1\n",
    "    isverbose = config.verbose or False\n",
    "\n",
    "    # verbose function\n",
    "    if isverbose:\n",
    "        verbose = verbose_func\n",
    "    else:\n",
    "        verbose = lambda x: None\n",
    "\n",
    "        # evaluate initial f(x) and df/dx\n",
    "    f, g = opfunc(x)\n",
    "\n",
    "    f_hist = [f]\n",
    "    currentFuncEval = 1\n",
    "    state.funcEval = state.funcEval + 1\n",
    "    p = g.shape[0]\n",
    "\n",
    "    # check optimality of initial point\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <= tolFun:\n",
    "        verbose(\"optimality condition below tolFun\")\n",
    "        return x, f_hist\n",
    "\n",
    "    # optimize for a max of maxIter iterations\n",
    "    nIter = 0\n",
    "    times = []\n",
    "    while nIter < maxIter:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # keep track of nb of iterations\n",
    "        nIter = nIter + 1\n",
    "        state.nIter = state.nIter + 1\n",
    "\n",
    "        ############################################################\n",
    "        ## compute gradient descent direction\n",
    "        ############################################################\n",
    "        if state.nIter == 1:\n",
    "            d = -g\n",
    "            old_dirs = []\n",
    "            old_stps = []\n",
    "            Hdiag = 1\n",
    "        else:\n",
    "            # do lbfgs update (update memory)\n",
    "            y = g - g_old\n",
    "            s = d * t\n",
    "            ys = dot(y, s)\n",
    "\n",
    "            if ys > 1e-10:\n",
    "                # updating memory\n",
    "                if len(old_dirs) == nCorrection:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    del old_dirs[0]\n",
    "                    del old_stps[0]\n",
    "\n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "\n",
    "                # update scale of initial Hessian approximation\n",
    "                Hdiag = ys / dot(y, y)\n",
    "\n",
    "            # compute the approximate (L-BFGS) inverse Hessian\n",
    "            # multiplied by the gradient\n",
    "            k = len(old_dirs)\n",
    "\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            ro = [0] * nCorrection\n",
    "            for i in range(k):\n",
    "                ro[i] = 1 / dot(old_stps[i], old_dirs[i])\n",
    "\n",
    "            # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            al = [0] * nCorrection\n",
    "\n",
    "            q = -g\n",
    "            for i in range(k - 1, -1, -1):\n",
    "                al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "                q = q - al[i] * old_stps[i]\n",
    "\n",
    "            # multiply by initial Hessian\n",
    "            r = q * Hdiag\n",
    "            for i in range(k):\n",
    "                be_i = dot(old_stps[i], r) * ro[i]\n",
    "                r += (al[i] - be_i) * old_dirs[i]\n",
    "\n",
    "            d = r\n",
    "            # final direction is in r/d (same object)\n",
    "\n",
    "        g_old = g\n",
    "        f_old = f\n",
    "\n",
    "        ############################################################\n",
    "        ## compute step length\n",
    "        ############################################################\n",
    "        # directional derivative\n",
    "        gtd = dot(g, d)\n",
    "\n",
    "        # check that progress can be made along that direction\n",
    "        if gtd > -tolX:\n",
    "            verbose(\"Can not make progress along direction.\")\n",
    "            break\n",
    "\n",
    "        # reset initial guess for step size\n",
    "        if state.nIter == 1:\n",
    "            tmp1 = tf.abs(g)\n",
    "            t = min(1, 1 / tf.reduce_sum(tmp1))\n",
    "        else:\n",
    "            t = learningRate\n",
    "\n",
    "        # optional line search: user function\n",
    "        lsFuncEval = 0\n",
    "        if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "            # perform line search, using user function\n",
    "            f, g, x, t, lsFuncEval = lineSearch(opfunc, x, t, d, f, g, gtd,\n",
    "                                                lineSearchOpts)\n",
    "            f_hist.append(f)\n",
    "        else:\n",
    "            # no line search, simply move with fixed-step\n",
    "            x += t * d\n",
    "\n",
    "            if nIter != maxIter:\n",
    "                # re-evaluate function only if not in last iteration\n",
    "                # the reason we do this: in a stochastic setting,\n",
    "                # no use to re-evaluate that function here\n",
    "                f, g = opfunc(x)\n",
    "                lsFuncEval = 1\n",
    "                f_hist.append(f)\n",
    "\n",
    "        # update func eval\n",
    "        currentFuncEval = currentFuncEval + lsFuncEval\n",
    "        state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "        ############################################################\n",
    "        ## check conditions\n",
    "        ############################################################\n",
    "        if nIter == maxIter:\n",
    "            break\n",
    "\n",
    "        if currentFuncEval >= maxEval:\n",
    "            # max nb of function evals\n",
    "            verbose('max nb of function evals')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(g)\n",
    "        if tf.reduce_sum(tmp1) <= tolFun:\n",
    "            # check optimality\n",
    "            verbose('optimality condition below tolFun')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(d * t)\n",
    "        if tf.reduce_sum(tmp1) <= tolX:\n",
    "            # step size below tolX\n",
    "            verbose('step size below tolX')\n",
    "            break\n",
    "\n",
    "        if tf.abs(f - f_old) < tolX:\n",
    "            # function value changing less than tolX\n",
    "            verbose('function value changing less than tolX' +\n",
    "                    str(tf.abs(f - f_old)))\n",
    "            break\n",
    "\n",
    "        if do_verbose:\n",
    "            log_fn(nIter, f.numpy(), True)\n",
    "            #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "            record_time()\n",
    "            times.append(last_time())\n",
    "\n",
    "        if nIter == maxIter - 1:\n",
    "            final_loss = f.numpy()\n",
    "\n",
    "    # save state\n",
    "    state.old_dirs = old_dirs\n",
    "    state.old_stps = old_stps\n",
    "    state.Hdiag = Hdiag\n",
    "    state.g_old = g_old\n",
    "    state.f_old = f_old\n",
    "    state.t = t\n",
    "    state.d = d\n",
    "\n",
    "    return x, f_hist, currentFuncEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "        # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "#         self.u_model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal')) # activation = tf.nn.tanh,\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.nu = nu\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.t_f = tf.convert_to_tensor(X_f, dtype=self.dtype)\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_sum(tf.square(u - u_pred)) + tf.reduce_sum(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the input we’ll need later, t\n",
    "            tape.watch(self.t_f)\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(self.t_f)\n",
    "            # Deriving INSIDE the tape (since we’ll need the t derivative of this later, u_tt)\n",
    "            u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "        # Getting the other derivatives\n",
    "        u_tt = tape.gradient(u_t, self.t_f)\n",
    "\n",
    "        # Buidling the PINNs\n",
    "        return self.nu*np.sin(u) + u_tt #self.nu*u + u_tt\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "#             print(loss_value)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.u_model.trainable_variables))\n",
    "#             self.logger.log_train_epoch(epoch, loss_value)\n",
    "            if epoch % 10 == 0:\n",
    "                plt.clf()\n",
    "                plt.scatter(X_u, u, marker='.')\n",
    "                plt.scatter(X_u, self.u_model(X_u), marker='.')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "\n",
    "        lbfgs(loss_and_flat_grad,\n",
    "          self.get_weights(),\n",
    "          nt_config, Struct(), True,\n",
    "          lambda epoch, loss, is_iter:\n",
    "            self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Training and plotting the results\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "# x, t, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "X_f, Exact_u, X_u_train, u_train, lb, ub = prep_data(N_u, N_f, noise=0.01)\n",
    "# plt.plot(Exact_u, \".\")\n",
    "print(X_u_train.shape, u_train.shape)\n",
    "plt.scatter(X_u_train, u_train, marker='.')\n",
    "#plt.scatter(X_f, Exact_u, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_u_train, ub, lb, nu=OMEGA**2)\n",
    "def error():\n",
    "    u_pred, _ = pinn.predict(X_f)\n",
    "    return np.linalg.norm(Exact_u - u_pred, 2) / np.linalg.norm(Exact_u, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_f)\n",
    "plt.scatter(X_f, u_pred, marker='.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "#   Exact_u, X, T, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "Rmd,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
