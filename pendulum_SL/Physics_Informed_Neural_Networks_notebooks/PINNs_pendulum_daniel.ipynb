{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was written in Google Colaboratory.\n",
    "\n",
    "Here is the [direct link](https://colab.research.google.com/drive/1Um7lgU5-GDcyWnVL2jlFEkyeykIiScxj) to the Colaboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Mj-RBCn8MZ"
   },
   "source": [
    "# 0. Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al80f-lPoJjh"
   },
   "source": [
    "## Getting the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "r3iHOMsdnNiq",
    "outputId": "2cede2e0-86a1-4292-b784-5b3de1284a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PINNs'...\n",
      "remote: Enumerating objects: 736, done.\u001b[K\n",
      "remote: Total 736 (delta 0), reused 0 (delta 0), pack-reused 736\u001b[K\n",
      "Receiving objects: 100% (736/736), 474.47 MiB | 26.34 MiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n",
      "Checking out files: 100% (561/561), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maziarraissi/PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gtoc_dXgoOZq"
   },
   "source": [
    "## Setting up modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNMtDjXkFHaN"
   },
   "source": [
    "TeX packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TaZCKcDsEVRP",
    "outputId": "6a2eddf4-1f32-4045-9c70-76cae377fdc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 86.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package fonts-droid-fallback.\n",
      "(Reading database ... 144433 files and directories currently installed.)\n",
      "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
      "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
      "Selecting previously unselected package fonts-lato.\n",
      "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
      "Unpacking fonts-lato (2.0-2) ...\n",
      "Selecting previously unselected package poppler-data.\n",
      "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
      "Unpacking poppler-data (0.4.8-2) ...\n",
      "Selecting previously unselected package tex-common.\n",
      "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
      "Unpacking tex-common (6.09) ...\n",
      "Selecting previously unselected package libkpathsea6:amd64.\n",
      "Preparing to unpack .../04-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package libptexenc1:amd64.\n",
      "Preparing to unpack .../05-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package libsynctex1:amd64.\n",
      "Preparing to unpack .../06-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package libtexlua52:amd64.\n",
      "Preparing to unpack .../07-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package libtexluajit2:amd64.\n",
      "Preparing to unpack .../08-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package t1utils.\n",
      "Preparing to unpack .../09-t1utils_1.41-2_amd64.deb ...\n",
      "Unpacking t1utils (1.41-2) ...\n",
      "Selecting previously unselected package libcupsimage2:amd64.\n",
      "Preparing to unpack .../10-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
      "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
      "Selecting previously unselected package libijs-0.35:amd64.\n",
      "Preparing to unpack .../11-libijs-0.35_0.35-13_amd64.deb ...\n",
      "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
      "Selecting previously unselected package libjbig2dec0:amd64.\n",
      "Preparing to unpack .../12-libjbig2dec0_0.13-6_amd64.deb ...\n",
      "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
      "Selecting previously unselected package libgs9-common.\n",
      "Preparing to unpack .../13-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.12_all.deb ...\n",
      "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Selecting previously unselected package libgs9:amd64.\n",
      "Preparing to unpack .../14-libgs9_9.26~dfsg+0-0ubuntu0.18.04.12_amd64.deb ...\n",
      "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Selecting previously unselected package libpotrace0.\n",
      "Preparing to unpack .../15-libpotrace0_1.14-2_amd64.deb ...\n",
      "Unpacking libpotrace0 (1.14-2) ...\n",
      "Selecting previously unselected package libzzip-0-13:amd64.\n",
      "Preparing to unpack .../16-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package texlive-binaries.\n",
      "Preparing to unpack .../17-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
      "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Selecting previously unselected package ghostscript.\n",
      "Preparing to unpack .../18-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.12_amd64.deb ...\n",
      "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Selecting previously unselected package dvipng.\n",
      "Preparing to unpack .../19-dvipng_1.15-1_amd64.deb ...\n",
      "Unpacking dvipng (1.15-1) ...\n",
      "Selecting previously unselected package fonts-adf-accanthis.\n",
      "Preparing to unpack .../20-fonts-adf-accanthis_0.20110505-1_all.deb ...\n",
      "Unpacking fonts-adf-accanthis (0.20110505-1) ...\n",
      "Selecting previously unselected package fonts-adf-berenis.\n",
      "Preparing to unpack .../21-fonts-adf-berenis_0.20110505-1_all.deb ...\n",
      "Unpacking fonts-adf-berenis (0.20110505-1) ...\n",
      "Selecting previously unselected package fonts-adf-gillius.\n",
      "Preparing to unpack .../22-fonts-adf-gillius_0.20110505-1_all.deb ...\n",
      "Unpacking fonts-adf-gillius (0.20110505-1) ...\n",
      "Selecting previously unselected package fonts-adf-universalis.\n",
      "Preparing to unpack .../23-fonts-adf-universalis_0.20110505-1_all.deb ...\n",
      "Unpacking fonts-adf-universalis (0.20110505-1) ...\n",
      "Selecting previously unselected package fonts-cabin.\n",
      "Preparing to unpack .../24-fonts-cabin_1.5-2_all.deb ...\n",
      "Unpacking fonts-cabin (1.5-2) ...\n",
      "Selecting previously unselected package fonts-comfortaa.\n",
      "Preparing to unpack .../25-fonts-comfortaa_3.001-2_all.deb ...\n",
      "Unpacking fonts-comfortaa (3.001-2) ...\n",
      "Selecting previously unselected package fonts-croscore.\n",
      "Preparing to unpack .../26-fonts-croscore_20171026-2_all.deb ...\n",
      "Unpacking fonts-croscore (20171026-2) ...\n",
      "Selecting previously unselected package fonts-crosextra-caladea.\n",
      "Preparing to unpack .../27-fonts-crosextra-caladea_20130214-2_all.deb ...\n",
      "Unpacking fonts-crosextra-caladea (20130214-2) ...\n",
      "Selecting previously unselected package fonts-crosextra-carlito.\n",
      "Preparing to unpack .../28-fonts-crosextra-carlito_20130920-1_all.deb ...\n",
      "Unpacking fonts-crosextra-carlito (20130920-1) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../29-fonts-dejavu-core_2.37-1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-1) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../30-fonts-dejavu-extra_2.37-1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
      "Selecting previously unselected package fonts-ebgaramond.\n",
      "Preparing to unpack .../31-fonts-ebgaramond_0.016-1_all.deb ...\n",
      "Unpacking fonts-ebgaramond (0.016-1) ...\n",
      "Selecting previously unselected package fonts-ebgaramond-extra.\n",
      "Preparing to unpack .../32-fonts-ebgaramond-extra_0.016-1_all.deb ...\n",
      "Unpacking fonts-ebgaramond-extra (0.016-1) ...\n",
      "Selecting previously unselected package fonts-font-awesome.\n",
      "Preparing to unpack .../33-fonts-font-awesome_4.7.0~dfsg-3_all.deb ...\n",
      "Unpacking fonts-font-awesome (4.7.0~dfsg-3) ...\n",
      "Selecting previously unselected package fonts-freefont-otf.\n",
      "Preparing to unpack .../34-fonts-freefont-otf_20120503-7_all.deb ...\n",
      "Unpacking fonts-freefont-otf (20120503-7) ...\n",
      "Selecting previously unselected package fonts-freefont-ttf.\n",
      "Preparing to unpack .../35-fonts-freefont-ttf_20120503-7_all.deb ...\n",
      "Unpacking fonts-freefont-ttf (20120503-7) ...\n",
      "Selecting previously unselected package fonts-gfs-artemisia.\n",
      "Preparing to unpack .../36-fonts-gfs-artemisia_1.1-5_all.deb ...\n",
      "Unpacking fonts-gfs-artemisia (1.1-5) ...\n",
      "Selecting previously unselected package fonts-gfs-complutum.\n",
      "Preparing to unpack .../37-fonts-gfs-complutum_1.1-6_all.deb ...\n",
      "Unpacking fonts-gfs-complutum (1.1-6) ...\n",
      "Selecting previously unselected package fonts-gfs-didot.\n",
      "Preparing to unpack .../38-fonts-gfs-didot_1.1-6_all.deb ...\n",
      "Unpacking fonts-gfs-didot (1.1-6) ...\n",
      "Selecting previously unselected package fonts-gfs-neohellenic.\n",
      "Preparing to unpack .../39-fonts-gfs-neohellenic_1.1-6_all.deb ...\n",
      "Unpacking fonts-gfs-neohellenic (1.1-6) ...\n",
      "Selecting previously unselected package fonts-gfs-olga.\n",
      "Preparing to unpack .../40-fonts-gfs-olga_1.1-5_all.deb ...\n",
      "Unpacking fonts-gfs-olga (1.1-5) ...\n",
      "Selecting previously unselected package fonts-gfs-solomos.\n",
      "Preparing to unpack .../41-fonts-gfs-solomos_1.1-5_all.deb ...\n",
      "Unpacking fonts-gfs-solomos (1.1-5) ...\n",
      "Selecting previously unselected package fonts-go.\n",
      "Preparing to unpack .../42-fonts-go_0~20161116-1_all.deb ...\n",
      "Unpacking fonts-go (0~20161116-1) ...\n",
      "Selecting previously unselected package fonts-junicode.\n",
      "Preparing to unpack .../43-fonts-junicode_1.001-2_all.deb ...\n",
      "Unpacking fonts-junicode (1.001-2) ...\n",
      "Selecting previously unselected package fonts-linuxlibertine.\n",
      "Preparing to unpack .../44-fonts-linuxlibertine_5.3.0-4_all.deb ...\n",
      "Unpacking fonts-linuxlibertine (5.3.0-4) ...\n",
      "Selecting previously unselected package fonts-lmodern.\n",
      "Preparing to unpack .../45-fonts-lmodern_2.004.5-3_all.deb ...\n",
      "Unpacking fonts-lmodern (2.004.5-3) ...\n",
      "Selecting previously unselected package fonts-lobster.\n",
      "Preparing to unpack .../46-fonts-lobster_2.0-2_all.deb ...\n",
      "Unpacking fonts-lobster (2.0-2) ...\n",
      "Selecting previously unselected package fonts-lobstertwo.\n",
      "Preparing to unpack .../47-fonts-lobstertwo_2.0-2_all.deb ...\n",
      "Unpacking fonts-lobstertwo (2.0-2) ...\n",
      "Selecting previously unselected package fonts-noto-hinted.\n",
      "Preparing to unpack .../48-fonts-noto-hinted_20171026-2_all.deb ...\n",
      "Unpacking fonts-noto-hinted (20171026-2) ...\n",
      "Selecting previously unselected package fonts-noto-mono.\n",
      "Preparing to unpack .../49-fonts-noto-mono_20171026-2_all.deb ...\n",
      "Unpacking fonts-noto-mono (20171026-2) ...\n",
      "Selecting previously unselected package fonts-oflb-asana-math.\n",
      "Preparing to unpack .../50-fonts-oflb-asana-math_000.907-6_all.deb ...\n",
      "Unpacking fonts-oflb-asana-math (000.907-6) ...\n",
      "Selecting previously unselected package fonts-open-sans.\n",
      "Preparing to unpack .../51-fonts-open-sans_1.11-1_all.deb ...\n",
      "Unpacking fonts-open-sans (1.11-1) ...\n",
      "Selecting previously unselected package fonts-roboto-hinted.\n",
      "Preparing to unpack .../52-fonts-roboto-hinted_2%3a0~20160106-2_all.deb ...\n",
      "Unpacking fonts-roboto-hinted (2:0~20160106-2) ...\n",
      "Selecting previously unselected package fonts-sil-gentium.\n",
      "Preparing to unpack .../53-fonts-sil-gentium_20081126%3a1.03-2_all.deb ...\n",
      "Unpacking fonts-sil-gentium (20081126:1.03-2) ...\n",
      "Selecting previously unselected package fonts-sil-gentium-basic.\n",
      "Preparing to unpack .../54-fonts-sil-gentium-basic_1.102-1_all.deb ...\n",
      "Unpacking fonts-sil-gentium-basic (1.102-1) ...\n",
      "Selecting previously unselected package fonts-sil-gentiumplus.\n",
      "Preparing to unpack .../55-fonts-sil-gentiumplus_5.000-2_all.deb ...\n",
      "Unpacking fonts-sil-gentiumplus (5.000-2) ...\n",
      "Selecting previously unselected package fonts-sil-gentiumplus-compact.\n",
      "Preparing to unpack .../56-fonts-sil-gentiumplus-compact_5.000-2_all.deb ...\n",
      "Unpacking fonts-sil-gentiumplus-compact (5.000-2) ...\n",
      "Selecting previously unselected package fonts-texgyre.\n",
      "Preparing to unpack .../57-fonts-texgyre_20160520-1_all.deb ...\n",
      "Unpacking fonts-texgyre (20160520-1) ...\n",
      "Selecting previously unselected package gsfonts.\n",
      "Preparing to unpack .../58-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
      "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
      "Selecting previously unselected package javascript-common.\n",
      "Preparing to unpack .../59-javascript-common_11_all.deb ...\n",
      "Unpacking javascript-common (11) ...\n",
      "Selecting previously unselected package libcupsfilters1:amd64.\n",
      "Preparing to unpack .../60-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
      "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
      "Selecting previously unselected package libjs-jquery.\n",
      "Preparing to unpack .../61-libjs-jquery_3.2.1-1_all.deb ...\n",
      "Unpacking libjs-jquery (3.2.1-1) ...\n",
      "Selecting previously unselected package rubygems-integration.\n",
      "Preparing to unpack .../62-rubygems-integration_1.11_all.deb ...\n",
      "Unpacking rubygems-integration (1.11) ...\n",
      "Selecting previously unselected package ruby2.5.\n",
      "Preparing to unpack .../63-ruby2.5_2.5.1-1ubuntu1.6_amd64.deb ...\n",
      "Unpacking ruby2.5 (2.5.1-1ubuntu1.6) ...\n",
      "Selecting previously unselected package ruby.\n",
      "Preparing to unpack .../64-ruby_1%3a2.5.1_amd64.deb ...\n",
      "Unpacking ruby (1:2.5.1) ...\n",
      "Selecting previously unselected package rake.\n",
      "Preparing to unpack .../65-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
      "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
      "Selecting previously unselected package ruby-did-you-mean.\n",
      "Preparing to unpack .../66-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
      "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
      "Selecting previously unselected package ruby-minitest.\n",
      "Preparing to unpack .../67-ruby-minitest_5.10.3-1_all.deb ...\n",
      "Unpacking ruby-minitest (5.10.3-1) ...\n",
      "Selecting previously unselected package ruby-net-telnet.\n",
      "Preparing to unpack .../68-ruby-net-telnet_0.1.1-2_all.deb ...\n",
      "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
      "Selecting previously unselected package ruby-power-assert.\n",
      "Preparing to unpack .../69-ruby-power-assert_0.3.0-1_all.deb ...\n",
      "Unpacking ruby-power-assert (0.3.0-1) ...\n",
      "Selecting previously unselected package ruby-test-unit.\n",
      "Preparing to unpack .../70-ruby-test-unit_3.2.5-1_all.deb ...\n",
      "Unpacking ruby-test-unit (3.2.5-1) ...\n",
      "Selecting previously unselected package libruby2.5:amd64.\n",
      "Preparing to unpack .../71-libruby2.5_2.5.1-1ubuntu1.6_amd64.deb ...\n",
      "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.6) ...\n",
      "Selecting previously unselected package lmodern.\n",
      "Preparing to unpack .../72-lmodern_2.004.5-3_all.deb ...\n",
      "Unpacking lmodern (2.004.5-3) ...\n",
      "Selecting previously unselected package preview-latex-style.\n",
      "Preparing to unpack .../73-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
      "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
      "Selecting previously unselected package tex-gyre.\n",
      "Preparing to unpack .../74-tex-gyre_20160520-1_all.deb ...\n",
      "Unpacking tex-gyre (20160520-1) ...\n",
      "Selecting previously unselected package texlive-base.\n",
      "Preparing to unpack .../75-texlive-base_2017.20180305-1_all.deb ...\n",
      "Unpacking texlive-base (2017.20180305-1) ...\n",
      "Selecting previously unselected package texlive-fonts-extra.\n",
      "Preparing to unpack .../76-texlive-fonts-extra_2017.20180305-2_all.deb ...\n",
      "Unpacking texlive-fonts-extra (2017.20180305-2) ...\n",
      "Selecting previously unselected package fonts-stix.\n",
      "Preparing to unpack .../77-fonts-stix_1.1.1-4_all.deb ...\n",
      "Unpacking fonts-stix (1.1.1-4) ...\n",
      "Selecting previously unselected package texlive-fonts-extra-links.\n",
      "Preparing to unpack .../78-texlive-fonts-extra-links_2017.20180305-2_all.deb ...\n",
      "Unpacking texlive-fonts-extra-links (2017.20180305-2) ...\n",
      "Selecting previously unselected package texlive-fonts-recommended.\n",
      "Preparing to unpack .../79-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
      "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
      "Selecting previously unselected package texlive-latex-base.\n",
      "Preparing to unpack .../80-texlive-latex-base_2017.20180305-1_all.deb ...\n",
      "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
      "Selecting previously unselected package texlive-latex-recommended.\n",
      "Preparing to unpack .../81-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
      "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
      "Selecting previously unselected package texlive-pictures.\n",
      "Preparing to unpack .../82-texlive-pictures_2017.20180305-1_all.deb ...\n",
      "Unpacking texlive-pictures (2017.20180305-1) ...\n",
      "Selecting previously unselected package texlive-latex-extra.\n",
      "Preparing to unpack .../83-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
      "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
      "Selecting previously unselected package texlive-plain-generic.\n",
      "Preparing to unpack .../84-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
      "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
      "Selecting previously unselected package tipa.\n",
      "Preparing to unpack .../85-tipa_2%3a1.3-20_all.deb ...\n",
      "Unpacking tipa (2:1.3-20) ...\n",
      "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Setting up libjs-jquery (3.2.1-1) ...\n",
      "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
      "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Setting up fonts-gfs-neohellenic (1.1-6) ...\n",
      "Setting up tex-common (6.09) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "update-language: texlive-base not installed and configured, doing nothing!\n",
      "Setting up fonts-stix (1.1.1-4) ...\n",
      "Setting up fonts-comfortaa (3.001-2) ...\n",
      "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
      "Setting up fonts-dejavu-core (2.37-1) ...\n",
      "Setting up fonts-linuxlibertine (5.3.0-4) ...\n",
      "Setting up poppler-data (0.4.8-2) ...\n",
      "Setting up fonts-oflb-asana-math (000.907-6) ...\n",
      "Setting up tex-gyre (20160520-1) ...\n",
      "Setting up fonts-lobster (2.0-2) ...\n",
      "Setting up fonts-gfs-solomos (1.1-5) ...\n",
      "Setting up fonts-adf-accanthis (0.20110505-1) ...\n",
      "Setting up fonts-freefont-otf (20120503-7) ...\n",
      "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
      "Setting up fonts-open-sans (1.11-1) ...\n",
      "Setting up fonts-texgyre (20160520-1) ...\n",
      "Setting up fonts-crosextra-carlito (20130920-1) ...\n",
      "Setting up fonts-ebgaramond-extra (0.016-1) ...\n",
      "Setting up fonts-font-awesome (4.7.0~dfsg-3) ...\n",
      "Setting up fonts-junicode (1.001-2) ...\n",
      "Setting up fonts-noto-mono (20171026-2) ...\n",
      "Setting up fonts-lato (2.0-2) ...\n",
      "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
      "Setting up fonts-gfs-complutum (1.1-6) ...\n",
      "Setting up fonts-cabin (1.5-2) ...\n",
      "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
      "Setting up fonts-sil-gentiumplus-compact (5.000-2) ...\n",
      "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
      "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
      "Setting up fonts-adf-gillius (0.20110505-1) ...\n",
      "Setting up fonts-crosextra-caladea (20130214-2) ...\n",
      "Setting up fonts-noto-hinted (20171026-2) ...\n",
      "Setting up t1utils (1.41-2) ...\n",
      "Setting up fonts-ebgaramond (0.016-1) ...\n",
      "Setting up ruby-net-telnet (0.1.1-2) ...\n",
      "Setting up fonts-croscore (20171026-2) ...\n",
      "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
      "Setting up rubygems-integration (1.11) ...\n",
      "Setting up libpotrace0 (1.14-2) ...\n",
      "Setting up fonts-adf-berenis (0.20110505-1) ...\n",
      "Setting up fonts-adf-universalis (0.20110505-1) ...\n",
      "Setting up fonts-sil-gentiumplus (5.000-2) ...\n",
      "Setting up fonts-gfs-didot (1.1-6) ...\n",
      "Setting up javascript-common (11) ...\n",
      "Setting up fonts-gfs-artemisia (1.1-5) ...\n",
      "Setting up fonts-dejavu-extra (2.37-1) ...\n",
      "Setting up fonts-freefont-ttf (20120503-7) ...\n",
      "Setting up ruby-minitest (5.10.3-1) ...\n",
      "Setting up fonts-go (0~20161116-1) ...\n",
      "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
      "Setting up fonts-sil-gentium (20081126:1.03-2) ...\n",
      "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Setting up fonts-sil-gentium-basic (1.102-1) ...\n",
      "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
      "Setting up fonts-gfs-olga (1.1-5) ...\n",
      "Setting up fonts-lmodern (2.004.5-3) ...\n",
      "Setting up ruby-power-assert (0.3.0-1) ...\n",
      "Setting up fonts-roboto-hinted (2:0~20160106-2) ...\n",
      "Setting up fonts-lobstertwo (2.0-2) ...\n",
      "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.12) ...\n",
      "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
      "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
      "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
      "Setting up texlive-base (2017.20180305-1) ...\n",
      "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
      "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
      "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
      "mktexlsr: Done.\n",
      "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
      "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
      "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
      "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up texlive-fonts-extra-links (2017.20180305-2) ...\n",
      "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
      "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
      "Setting up texlive-latex-base (2017.20180305-1) ...\n",
      "Setting up lmodern (2.004.5-3) ...\n",
      "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
      "Setting up texlive-fonts-extra (2017.20180305-2) ...\n",
      "Setting up texlive-pictures (2017.20180305-1) ...\n",
      "Setting up dvipng (1.15-1) ...\n",
      "Setting up tipa (2:1.3-20) ...\n",
      "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
      "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
      "update-fmtutil has updated the following file(s):\n",
      "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
      "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
      "If you want to activate the changes in the above file(s),\n",
      "you should run fmtutil-sys or fmtutil.\n",
      "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
      "Setting up ruby-test-unit (3.2.5-1) ...\n",
      "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.6) ...\n",
      "Setting up ruby2.5 (2.5.1-1ubuntu1.6) ...\n",
      "Setting up ruby (1:2.5.1) ...\n",
      "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for tex-common (6.09) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Running updmap-sys. This may take some time... done.\n",
      "Running mktexlsr /var/lib/texmf ... done.\n",
      "Building format(s) --all.\n",
      "\tThis may take some time... done.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -qq install texlive-fonts-recommended texlive-fonts-extra dvipng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Otc4Ap7qFMlf"
   },
   "source": [
    "Pip modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "r0HJlRB1SePz",
    "outputId": "321a6b90-74f8-4d7a-ef94-981dbe9695be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (0.3.3)\n",
      "Collecting latex\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/f3/c2562ee509faadaaf4f9d5b9491de146c6522ed2843dcecfd4f8e1a72f1d/latex-0.7.0.tar.gz\n",
      "Collecting tempdir\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/b2/b931869a9f9ad9fa14deecbcfc28e514b0755f8b904d9fe48864951b1a60/tempdir-0.7.1.tar.gz\n",
      "Collecting data\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/e9/623be82fac4250fc614741f5b1ead83d339794f94b19ac8665b6ea12ee05/data-0.4.tar.gz\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from latex) (0.16.0)\n",
      "Collecting shutilwhich\n",
      "  Downloading https://files.pythonhosted.org/packages/66/be/783f181594bb8bcfde174d6cd1e41956b986d0d8d337d535eb2555b92f8d/shutilwhich-1.1.0.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from data->latex) (1.12.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from data->latex) (4.4.2)\n",
      "Collecting funcsigs\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: latex, tempdir, data, shutilwhich\n",
      "  Building wheel for latex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for latex: filename=latex-0.7.0-cp36-none-any.whl size=7605 sha256=9e734cf2d1a6931f8bdc6016b2cc8e324de784abd5422c2f71a58587ab833531\n",
      "  Stored in directory: /root/.cache/pip/wheels/a5/00/74/5aed853dec3fbc45e9c9cf4949f664f1bd8b6f3e6def432019\n",
      "  Building wheel for tempdir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tempdir: filename=tempdir-0.7.1-cp36-none-any.whl size=2214 sha256=707ef19698e60a9e882837c35f4072da0dd9db5948ba21f3976d1cd9be388f29\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/f2/40/6806964a69dfe3e46eff51b06a9a036af80b933479536fa295\n",
      "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for data: filename=data-0.4-cp36-none-any.whl size=7247 sha256=8db5eec3f492e8beba2f88a2cc54c22040bc1d34f382d288cbd3ec10f2082a9f\n",
      "  Stored in directory: /root/.cache/pip/wheels/f6/09/e5/5afbc45fb0de471541092c7cd8b48d3483be68b1890955e6b9\n",
      "  Building wheel for shutilwhich (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-cp36-none-any.whl size=2782 sha256=e60399b5fd418fb5b2968a637725b7eb8fc27911c3cedd9cba419025e9617532\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/ef/eb/27a815601fa45bbb0301dae456f0853700502233b801fe4bb1\n",
      "Successfully built latex tempdir data shutilwhich\n",
      "Installing collected packages: tempdir, funcsigs, data, shutilwhich, latex\n",
      "Successfully installed data-0.4 funcsigs-1.0.2 latex-0.7.0 shutilwhich-1.1.0 tempdir-0.7.1\n",
      "Collecting PyDvi\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/8c/501abbb16c50bf5420a166b8c64fbe5f75bca2726d1bbe5dd48e971cc38a/PyDvi-0.1.0.tar.gz (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 9.4MB/s \n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25hCollecting ghostscript\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/bd/c96072da3a98a498285382ec6ee4b18379c021e4bb6561308e25bc4c04d0/ghostscript-0.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ghostscript) (46.3.0)\n",
      "Installing collected packages: ghostscript\n",
      "Successfully installed ghostscript-0.6\n",
      "Collecting pyDOE\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/ac/91fe4c039e2744466621343d3b8af4a485193ed0aab53af5b1db03be0989/pyDOE-0.3.8.zip\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyDOE) (1.4.1)\n",
      "Building wheels for collected packages: pyDOE\n",
      "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyDOE: filename=pyDOE-0.3.8-cp36-none-any.whl size=18178 sha256=cd727b8435650c4f06177a60936fb16ad67a35355eda30f0b3178a259db2fe35\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/c8/58/a6493bd415e8ba5735082b5e0c096d7c1f2933077a8ce34544\n",
      "Successfully built pyDOE\n",
      "Installing collected packages: pyDOE\n",
      "Successfully installed pyDOE-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gast==0.3.3\n",
    "!pip install latex\n",
    "!pip install PyDvi\n",
    "!pip install ghostscript\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "colab_type": "code",
    "id": "srpq4aQNoQ1E",
    "outputId": "abf76cea-88d7-43d7-b1b1-6413b57ceffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.2.0:\n",
      "  Successfully uninstalled tensorflow-2.2.0\n",
      "Collecting tensorflow-gpu==2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bf/c28971266ca854a64f4b26f07c4112ddd61f30b4d1f18108b954a746f8ea/tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 31kB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.18.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (3.2.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (3.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.28.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu==2.2.0) (46.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.2.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.6.0.post3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2.9)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.4.8)\n",
      "Installing collected packages: tensorflow-gpu\n",
      "Successfully installed tensorflow-gpu-2.2.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow-gpu==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksKujMvUFRNW"
   },
   "source": [
    "## Imports, config, and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEDn2fqlqctT",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9dXWQJdWRHGO",
    "outputId": "f101f2fb-0ed0-4385-ebe4-e40befb8f004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.66972348, -0.18611859, -0.93054407, -0.99945953, -0.43887534,\n",
       "       -0.94393153,  0.04099598, -0.15406325, -0.97855032, -0.99961322,\n",
       "       -0.71721061, -0.92917048, -0.23932536, -0.99584506, -0.67262181,\n",
       "       -0.75591637, -0.44002113,  0.65846305, -0.73713438, -0.92302402,\n",
       "       -0.99987942, -0.9856168 , -0.98924549, -0.99902213, -0.83456146,\n",
       "       -0.98859131, -0.9901342 , -0.99330519, -0.99391569, -0.99392961,\n",
       "       -0.99617438, -0.84645164, -0.99466989, -0.93876847, -0.99994024,\n",
       "       -0.99368466, -0.82695033,  0.49655037, -0.90367021, -0.9909906 ,\n",
       "       -0.88038776,  0.15078198, -0.99923598, -0.88516046, -0.99736836,\n",
       "       -0.71117262, -0.95005122, -0.7750998 , -0.99079039, -0.69677709,\n",
       "       -0.93067611, -0.98373064, -0.01160027, -0.95403245, -0.99794872,\n",
       "       -0.45976314,  0.24694394, -0.61597698, -0.99475159,  0.56261442,\n",
       "       -0.93788833,  0.70906992,  0.96965831, -0.9932513 ,  0.64038272,\n",
       "        0.13041727,  0.84970917, -0.97807311,  0.48250592,  0.95793797,\n",
       "       -0.7430881 , -0.88292331, -0.87572541, -0.91012712, -0.53722941,\n",
       "        0.7712163 , -0.82156124,  0.71505353, -0.9910539 ,  0.93855979,\n",
       "       -0.97737473, -0.52529096,  0.48209657,  0.01257232,  0.98473173,\n",
       "       -0.80327655, -0.58543543,  0.98432054,  0.51683949,  0.92134364,\n",
       "       -0.26555333,  0.78712466,  0.62550772,  0.94760357, -0.10402746,\n",
       "        0.15595634, -0.94725483,  0.53216189,  0.32911601,  0.91793232,\n",
       "        0.9986573 ,  0.28064456, -0.99434899,  0.48576407, -0.27706024,\n",
       "        0.92044953,  0.98257426, -0.99063119, -0.29122322,  0.96694908,\n",
       "       -0.19260993,  0.09859783,  0.68543054, -0.7989796 ,  0.79516873,\n",
       "       -0.99991581,  0.79771128,  0.29228795, -0.7620112 ,  0.99599556,\n",
       "        0.93922805, -0.79076733,  0.99795234,  0.90714803,  0.50528449,\n",
       "        0.57000624,  0.7557943 ,  0.99993284,  0.99030962,  0.53725615,\n",
       "        0.10749853, -0.17179281, -0.37108172,  0.92269045,  0.18331463,\n",
       "        0.67540709, -0.21321575,  0.92253788,  0.99995047,  0.65542937,\n",
       "        0.99259638,  0.99976019,  0.99985392,  0.9960051 , -0.97090298,\n",
       "        0.72463946,  0.99005192,  0.99999792,  0.92244991,  0.91679605,\n",
       "        0.99420956,  0.9999996 ,  0.99858908,  0.99769049,  0.9754736 ,\n",
       "        0.99588269,  0.99829066,  0.99921004,  0.99878616,  0.99999853,\n",
       "        0.99896373,  0.99835046,  0.99883995,  0.99968522,  0.99986169,\n",
       "        0.9979612 ,  0.99916748,  0.99891235,  0.99886295,  0.99988359,\n",
       "        0.99915007,  0.9992879 ,  0.99988973,  0.99938272,  0.99842802,\n",
       "        0.99784868,  0.81943856,  0.99787938,  0.99346182,  0.99879608,\n",
       "        0.99273959,  0.99559034,  0.99939297,  0.99583905,  0.99775227,\n",
       "        0.99718257,  0.99192366,  0.99783225,  0.99844628,  0.99386068,\n",
       "        0.99893589,  0.99929412,  0.9992054 , -0.96882989, -0.94949384,\n",
       "        0.99831212,  0.99929415, -0.98012732,  0.99269184,  0.99886311,\n",
       "        0.99739809,  0.99863893,  0.99823978,  0.99713097,  0.99857344,\n",
       "        0.99585328,  0.99914309,  0.99554819,  0.99588974,  0.99102914,\n",
       "        0.98909833,  0.9833704 ,  0.91014184,  0.97339635,  0.95832481,\n",
       "        0.9329304 , -0.98261204,  0.99794696,  0.97694621,  0.8951753 ,\n",
       "        0.96623864,  0.93300493,  0.6299985 ,  0.58637405,  0.99157468,\n",
       "        0.93663687,  0.99099453,  0.99441316,  0.99595812,  0.99912962,\n",
       "        0.99166066,  0.99758646,  0.99919476,  0.99616992,  0.99848307,\n",
       "        0.99802413,  0.99932378,  0.99701255,  0.993394  ,  0.99062941,\n",
       "        0.98732321,  0.99958386,  0.99965702,  0.99894132,  0.99952449,\n",
       "        0.99947697,  0.99858754,  0.99941347,  0.99862192,  0.99927144,\n",
       "        0.99903186,  0.99922444,  0.99947295,  0.99938359,  0.99977314,\n",
       "        0.99898503,  0.9987472 ,  0.99948381,  0.99910683,  0.99959716,\n",
       "        0.99902298,  0.99893717,  0.99918605,  0.99934964,  0.99946319,\n",
       "        0.99940348,  0.99954953,  0.99918348,  0.9991834 ,  0.99908044,\n",
       "        0.99933808,  0.99940111,  0.99957687,  0.99961462,  0.99950688,\n",
       "        0.99979908,  0.99999932,  0.99973955,  0.99998851,  0.99977855,\n",
       "        0.99968525,  0.99982312,  0.99957134,  0.99956068,  0.99987307,\n",
       "        0.99954046,  0.9999531 ,  0.99992512,  0.99998459,  0.99985802,\n",
       "        0.9998679 ,  0.99953576,  0.99992598,  0.9925854 ,  0.99996223,\n",
       "        0.99844658,  0.9999815 ,  0.958937  ,  0.99855784,  0.99283472,\n",
       "        0.99998924,  0.86572399,  0.71270652,  0.62438332,  0.98437514,\n",
       "        0.99996239,  0.99985297,  0.99903648,  0.95435036,  0.99942964,\n",
       "        0.99913447,  0.99993811, -0.94667014,  0.20600526,  0.18723141,\n",
       "       -0.29387816,  0.05189883,  0.99987615,  0.99997821,  0.99084569,\n",
       "        0.5574584 ,  0.89893811,  0.99581435,  0.90443399,  0.99989901,\n",
       "        0.99794309,  0.99926808,  0.99854299,  0.99981872,  0.96821541,\n",
       "        0.99735851, -0.35659889,  0.99970429,  0.9911083 ,  0.99996134,\n",
       "        0.7877819 ,  0.99997557,  0.99944338,  0.99999016,  0.99958532,\n",
       "        0.99938773,  0.99989811,  0.99990192,  0.99999977,  0.99974965,\n",
       "        0.8836082 ,  0.99988544,  0.99983578,  0.99517168,  0.99967811,\n",
       "        0.99998073,  0.99992945,  0.99996502,  0.99991001,  0.99990156,\n",
       "        0.99999859,  0.99994392,  0.99975847,  0.99999229,  0.99971624,\n",
       "        0.99997398,  0.99996473,  0.99983115,  0.99981243,  0.99999606,\n",
       "        0.9997772 ,  0.99997059,  0.99993601,  0.99998438,  0.99993472,\n",
       "        0.99972664,  0.99987539,  0.99999761,  0.99994449,  0.99955787,\n",
       "        0.99983215,  0.99996026,  0.99906945,  0.99471771,  0.9998023 ,\n",
       "        0.9976442 ,  0.99896285,  0.998785  ,  0.99548084,  0.72628443,\n",
       "        0.9999564 ,  0.99987434,  0.99986927,  0.99987384,  0.99987828,\n",
       "        0.99988583,  0.99976116,  0.99970058,  0.99961887,  0.99979319,\n",
       "        0.99973249,  0.99981088,  0.9998462 ,  0.99977428,  0.99988039,\n",
       "        0.99973694,  0.99998191,  0.99963048,  0.99999447,  0.99998579,\n",
       "        0.99966878,  0.99999948,  0.99991156,  0.99997918,  0.99991156,\n",
       "        0.99998111,  0.99997347,  0.99994205,  0.9998959 ,  0.99990074,\n",
       "        0.99999198,  0.99986447,  0.99995374,  0.9999722 ,  0.99911608,\n",
       "        0.99999997,  0.99985948,  0.99997214,  0.99991233,  0.99995896,\n",
       "        0.99992333,  0.99986617,  0.99980989,  0.99967262,  0.99988256,\n",
       "        0.99988396,  0.99994525,  0.99965177,  0.99998761,  0.9997594 ,\n",
       "        0.99994086,  0.99997826,  0.99984292,  0.99968039,  0.99993597,\n",
       "        0.99996074,  0.99996821,  0.99985028,  0.99976623,  0.99980422,\n",
       "        0.99979186,  0.99999436,  0.99989144,  0.99984159,  0.99991707,\n",
       "        0.99974187,  0.99979129,  0.99972349,  0.99982713,  0.99989639,\n",
       "        0.9999988 ,  0.99984189,  0.99969881,  0.9998522 ,  0.99969254,\n",
       "        0.9998337 ,  0.99977798,  0.9996708 ,  0.99961091,  0.9997951 ,\n",
       "        0.99990576,  0.99973211,  0.99955357,  0.99997361,  0.99997264,\n",
       "        0.99965477,  0.9997773 ,  0.99993492,  0.99999555,  0.99976576,\n",
       "        0.99998654,  0.99980091,  0.99977142,  0.99974948,  0.99968189,\n",
       "        0.9998832 ,  0.99977477,  0.99960423,  0.99910047,  0.99982768,\n",
       "        0.99981654,  0.99983867,  0.99974533,  0.99958198,  0.99984874,\n",
       "        0.99952128,  0.99948655,  0.99971058,  0.99977984,  0.99989382,\n",
       "        0.99963424,  0.99941396,  0.99977917,  0.99953341,  0.9999994 ,\n",
       "        0.99978185,  0.99958141,  0.99950463,  0.99970707,  0.99988814,\n",
       "        0.99953409,  0.99992217,  0.99955646,  0.99948644,  0.99983626,\n",
       "        0.99978947,  0.99959595,  0.99978074,  0.99992985,  0.99964215,\n",
       "        0.999797  ,  0.99997359,  0.99941273,  0.99987774,  0.99970944,\n",
       "        0.99946608,  0.9995125 ,  0.52066849,  0.99976941,  0.99976376,\n",
       "        0.99987185,  0.99984358, -0.98168036,  0.99971419,  0.90020752,\n",
       "        0.99953885,  0.78688423, -0.56602781,  0.99999812,  0.99995559,\n",
       "        0.99960824,  0.94961671,  0.99978   ,  0.99998659,  0.99963173,\n",
       "        0.90950589,  0.99352696, -0.16890774,  0.99991092,  0.99321722,\n",
       "       -0.58556867,  0.99970254,  0.94681051,  0.64102174,  0.99988969,\n",
       "        0.94654944,  0.99996446,  0.89391329,  0.99961751, -0.20051403,\n",
       "        0.99988309,  0.26123535,  0.99996879,  0.99987055,  0.99789537,\n",
       "        0.99989264,  0.99984995,  0.99817729, -0.56211438,  0.99997224,\n",
       "        0.9999784 , -0.88236931,  0.99874466,  0.99963657,  0.92834826,\n",
       "       -0.47510054,  0.99994466,  0.98437022,  0.99721164,  0.99147471,\n",
       "        0.99612363,  0.1645832 ,  0.99671384,  0.96590369,  0.99320816,\n",
       "        0.7758704 ,  0.98559719,  0.99717909,  0.98421774,  0.99964127,\n",
       "       -0.48765929, -0.91820409, -0.98128931,  0.99984662,  0.59650419,\n",
       "        0.99989528,  0.98555583,  0.86263437,  0.88207258,  0.99490002,\n",
       "       -0.76128646,  0.9998919 ,  0.92145242,  0.9987345 ,  0.98366974,\n",
       "        0.99998891,  0.99721032,  0.04891039,  0.99915753,  0.99830815,\n",
       "       -0.10557656,  0.96848653,  0.99963018, -0.77898473,  0.98929721,\n",
       "        0.99304713,  0.98376769,  0.99994129,  0.99968557,  0.98136327,\n",
       "        0.91628904, -0.68432679,  0.96092598,  0.92049877,  0.99964328,\n",
       "        0.99952155,  0.80919516,  0.81625648,  0.99867596,  0.97948145,\n",
       "        0.99639016, -0.89205622,  0.60405417, -0.56486047, -0.22825247,\n",
       "       -0.99804152,  0.99591055, -0.84885902,  0.99998789,  0.59061354,\n",
       "        0.9857796 ,  0.99728443,  0.99945385,  0.99981319,  0.89685447,\n",
       "       -0.04238906,  0.9992941 ,  0.72393082,  0.99245165,  0.26494321,\n",
       "        0.12558419,  0.99539429,  0.99979632,  0.97276338,  0.98988309,\n",
       "        0.55635005,  0.33617615,  0.82147644,  0.06798924,  0.78514859,\n",
       "        0.84281412,  0.99999396, -0.70703831,  0.99835453,  0.69411342,\n",
       "        0.95621903,  0.99868036,  0.99981094, -0.11466468,  0.98548315,\n",
       "        0.99399029, -0.88673924,  0.98737448,  0.89071022,  0.96750397,\n",
       "        0.29977381,  0.99904407,  0.64181777,  0.99003874,  0.99974655,\n",
       "        0.97846983, -0.9723134 ,  0.89890065,  0.28853642, -0.91982035,\n",
       "       -0.61913384,  0.93793953,  0.9028737 ,  0.41891321,  0.92991697,\n",
       "        0.99092962,  0.92634878, -0.91203318,  0.93077846,  0.99945651,\n",
       "        0.99791168,  0.99825594,  0.99904922,  0.99927754,  0.89058645,\n",
       "        0.51005253,  0.89627731,  0.97111635,  0.99994936,  0.94130651,\n",
       "       -0.99959685,  0.05033199, -0.96183514,  0.96907511,  0.99899452,\n",
       "        0.99539582,  0.65820586,  0.99998655, -0.47707509,  0.33383892,\n",
       "       -0.93000423,  0.97187448,  0.20479694,  0.96315913,  0.99989991,\n",
       "        0.55241437,  0.99936908,  0.49403544,  0.65376576,  0.94423552,\n",
       "        0.22550719,  0.42414317,  0.99944102,  0.95557508,  0.99616281,\n",
       "        0.99915729,  0.27782617,  0.78260033,  0.87495512,  0.91809041,\n",
       "        0.94055143,  0.99367676, -0.89343292,  0.40311723,  0.93036964,\n",
       "        0.99408738,  0.98975212,  0.79838838,  0.63649941,  0.99457975,\n",
       "       -0.34747644,  0.99991361,  0.99872895,  0.99995091,  0.99980061,\n",
       "       -0.33146546,  0.94421393, -0.96451011,  0.59225038,  0.66007068,\n",
       "        0.99992169,  0.58019646, -0.99011445,  0.9976607 , -0.78000926,\n",
       "        0.97858664,  0.90015475,  0.9917721 , -0.33316735,  0.99438138,\n",
       "       -0.21480381, -0.60871963,  0.13439265,  0.84478796,  0.97960149,\n",
       "       -0.41154056,  0.9972006 ,  0.3200864 ,  0.67653169,  0.99570975,\n",
       "        0.99040556,  0.99270925,  0.99499256,  0.98459777,  0.96684094,\n",
       "        0.98813805,  0.86142911,  0.97136864,  0.99933606,  0.70761786,\n",
       "        0.85372008,  0.99477968,  0.99056193,  0.74789008,  0.9982873 ,\n",
       "        0.98698315, -0.25298262, -0.65759506,  0.99012221,  0.99812496,\n",
       "        0.995518  ,  0.94469966,  0.26717041,  0.97386128, -0.44201764,\n",
       "        0.9411936 ,  0.97657557,  0.98268828,  0.99663746, -0.92176227,\n",
       "        0.88333259, -0.05391414,  0.99985492,  0.99013093,  0.41105544,\n",
       "        0.97876938,  0.97604244,  0.94563213,  0.83430438,  0.9513353 ,\n",
       "        0.95979061,  0.88110566, -0.71360686, -0.98217625,  0.95231324,\n",
       "        0.99165307,  0.99973193,  0.99359751,  0.56286272, -0.99792752,\n",
       "        0.99968628,  0.99913994,  0.84806758,  0.99949222, -0.53532519,\n",
       "        0.90317642,  0.99050671,  0.93225564, -0.8882479 ,  0.99948248,\n",
       "       -0.85333479,  0.99974149,  0.95689101,  0.97946709,  0.99885127,\n",
       "        0.9363983 ,  0.97990279,  0.89042253,  0.97567559,  0.99346848,\n",
       "        0.98310696,  0.9998399 ,  0.55210149,  0.99812597,  0.93703431,\n",
       "        0.9816921 ,  0.56672302,  0.99726513, -0.84527007,  0.64777535,\n",
       "        0.99980838, -0.35972344,  0.98631517,  0.99487742,  0.02865828,\n",
       "       -0.12983296,  0.99054511,  0.99806358,  0.96317419,  0.99957391,\n",
       "        0.99889668,  0.99991432,  0.99971489,  0.99979978,  0.99073126,\n",
       "        0.99576103,  0.99993649,  0.99999911,  0.99685071,  0.99992012,\n",
       "        0.99998517,  0.9999959 ,  0.99998503,  0.99998965,  1.        ,\n",
       "        0.7490741 , -0.95916101,  0.58720063,  0.99991859,  0.99937077,\n",
       "        0.9998091 ,  0.99995365,  0.99972998,  0.9997644 ,  0.99891128,\n",
       "        0.99977452,  0.99986318,  0.99973112,  0.9997491 ,  0.99931039,\n",
       "        0.99972178,  0.99853585,  0.99850794,  0.9999543 ,  0.99954175,\n",
       "        0.99968617,  0.99911393,  0.9992495 ,  0.99948233,  0.99837552,\n",
       "        0.99893222,  0.9989063 ,  0.99802245,  0.99741338,  0.83522471,\n",
       "        0.99674546,  0.99785618,  0.99793416,  0.99575648,  0.98364018,\n",
       "        0.99568654,  0.99371858,  0.99296045,  0.99385265,  0.99558783,\n",
       "        0.98883848,  0.99816592,  0.98743846,  0.99760462,  0.99152211,\n",
       "        0.74112298,  0.99438826,  0.99798597,  0.99847563,  0.99548756,\n",
       "        0.999018  , -0.24289753,  0.99804122,  0.99382055,  0.99473013,\n",
       "        0.99174398,  0.99814134,  0.99278951,  0.99949084,  0.80630046,\n",
       "        0.99683735,  0.93041319,  0.81707216,  0.84917891,  0.98155255,\n",
       "       -0.04823648,  0.8393737 ,  0.76826803,  0.90785101,  0.90232533,\n",
       "       -0.90445415,  0.82244417,  0.99999936, -0.42430824,  0.99979606,\n",
       "        0.99991239, -0.89383446,  0.99944981,  0.99968705,  0.99999668,\n",
       "        0.99980525,  0.99466193, -0.68962899,  0.99943535,  0.99999479,\n",
       "        0.74225276,  0.9947667 ,  0.9922063 ,  0.98906282,  0.80898763,\n",
       "        0.9928387 ,  0.99814136,  0.88056554, -0.73084591,  0.99930847,\n",
       "        0.95335287,  0.99457103,  0.99934216,  0.99634297,  0.56460476,\n",
       "        0.99993762,  0.55516708,  0.99933954,  0.99889449, -0.14788812,\n",
       "       -0.99044139, -0.35225695, -0.99021065,  0.99996523,  0.67035492,\n",
       "        0.29549087, -0.98489829,  0.98771374,  0.53194349,  0.94017884,\n",
       "       -0.09382208,  0.99906267, -0.89618723,  0.98090328,  0.99861895,\n",
       "        0.72677219,  0.70397541,  0.96419139,  0.00708795, -0.98930411])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n",
    "npz = np.load('pendulum_data.npz', allow_pickle=True)\n",
    "lst = npz.files\n",
    "\n",
    "# Current state of the pendulum\n",
    "# Shape of (1000,3) contains cos(theta), sin(theta), theta_dot\n",
    "# getting theta values from inverse cosine of the observation space\n",
    "states = npz['states'][:, 0]\n",
    "# theta = [np.arccos(x) for x in states[0]]\n",
    "states\n",
    "# Time measurements 0.05 second interval\n",
    "# Shape of (2,), [0] contains list of time, [1] contains time step\n",
    "# t = npz['times']\n",
    "# theta = npz['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODGREPvZpqUz"
   },
   "source": [
    "burgersutil.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgPvqJiYFnYG",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
    "repoPath = os.path.join(\".\", \"PINNs\")\n",
    "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
    "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
    "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
    "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
    "\n",
    "sys.path.insert(0, utilsPath)\n",
    "from plotting import newfig, savefig\n",
    "\n",
    "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
    "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
    "    data = np.load('pendulum_with_time_data.npz', allow_pickle=True)\n",
    "\n",
    "    # Extracting data \n",
    "    t = data['times'] # 1000 element long array\n",
    "    theta = data['theta'] # 1000 element long array\n",
    "    \n",
    "    #---------------- Unnecessary ----------------#\n",
    "    # Reducing unnecessary variable\n",
    "    # # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
    "    # Exact_u = np.real(data['usol']).T # T x N\n",
    "\n",
    "    # Never entered\n",
    "    # if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
    "    #   dt = t[idx_t_1] - t[idx_t_0]\n",
    "    #   idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
    "    #   x_0 = x[idx_x,:]\n",
    "    #   u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
    "    #   u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "        \n",
    "    #   # Boudanry data\n",
    "    #   x_1 = np.vstack((lb, ub))\n",
    "      \n",
    "    #   # Test data\n",
    "    #   x_star = x\n",
    "    #   u_star = Exact_u[idx_t_1,:]\n",
    "\n",
    "    #   # Load IRK weights\n",
    "    #   tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "    #   IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
    "    #   IRK_times = tmp[q**2+q:]\n",
    "\n",
    "    #   return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
    "\n",
    "    # # Meshing x and t in 2D (256,100)\n",
    "    # X, T = np.meshgrid(x,t)\n",
    "\n",
    "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
    "    # X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "    # # Preparing the testing u_star\n",
    "    # u_star = Exact_u.flatten()[:,None]\n",
    "    \n",
    "    # Noiseless data TODO: add support for noisy data    \n",
    "    idx = np.random.choice(t.shape[0], N_u, replace=False)\n",
    "    t_train = t[idx]\n",
    "    theta_train = theta[idx]\n",
    "\n",
    "    #---------------- Unnecessary ----------------#\n",
    "    # if N_0 != None and N_1 != None:\n",
    "    #   Exact_u = Exact_u.T\n",
    "    #   idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
    "    #   x_0 = x[idx_x,:]\n",
    "    #   u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
    "    #   u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "          \n",
    "    #   idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
    "    #   x_1 = x[idx_x,:]\n",
    "    #   u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
    "    #   u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
    "      \n",
    "    #   dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
    "    #   q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
    "\n",
    "    #   # Load IRK weights\n",
    "    #   tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "    #   weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
    "    #   IRK_alpha = weights[0:-1,:]\n",
    "    #   IRK_beta = weights[-1:,:] \n",
    "    #   return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
    "\n",
    "    # if N_f == None:\n",
    "    #   lb = X_star.min(axis=0)\n",
    "    #   ub = X_star.max(axis=0) \n",
    "    #   return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "    lb = t.min(axis=0)\n",
    "    ub = t.max(axis=0) \n",
    "\n",
    "    #----- Boundary conditions are no longer necessary -----#\n",
    "    # # Getting the initial conditions (t=0)\n",
    "    # xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    # uu1 = Exact_u[0:1,:].T\n",
    "    # # Getting the lowest boundary conditions (x=-1) \n",
    "    # xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    # uu2 = Exact_u[:,0:1]\n",
    "    # # Getting the highest boundary conditions (x=1) \n",
    "    # xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    # uu3 = Exact_u[:,-1:]\n",
    "    # # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
    "    # X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    # u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "    # Generating the x and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
    "    t_f = lb + (ub-lb)*lhs(1, N_f)\n",
    "\n",
    "    #----- Already accomplished previously -----#\n",
    "    # # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    # idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    # # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
    "    # X_u_train = X_u_train[idx,:]\n",
    "    # # Getting the corresponding u_train\n",
    "    # u_train = u_train [idx,:]\n",
    "\n",
    "    # x and t => replaced by one t\n",
    "    # usol => replace by theta\n",
    "    # X and T => unnecessary\n",
    "    # Exact_u => unnecessary\n",
    "    # X_star => t\n",
    "    # u_star => theta\n",
    "    # X_u_train => t_train\n",
    "    # u_train => theta_train\n",
    "    # X_f => t_f\n",
    "    # ub same\n",
    "    # lb same\n",
    "\n",
    "    # Variables needed: X_f, ub, lb, X_star, u_star, X_u_train, u_train\n",
    "    return t, theta, t_train, theta_train, t_f, ub, lb\n",
    "\n",
    "class Logger(object):\n",
    "  def __init__(self, frequency=10):\n",
    "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "    self.start_time = time.time()\n",
    "    self.frequency = frequency\n",
    "\n",
    "  def __get_elapsed(self):\n",
    "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "  def __get_error_u(self):\n",
    "    return self.error_fn()\n",
    "\n",
    "  def set_error_fn(self, error_fn):\n",
    "    self.error_fn = error_fn\n",
    "  \n",
    "  def log_train_start(self, model):\n",
    "    print(\"\\nTraining started\")\n",
    "    print(\"================\")\n",
    "    self.model = model\n",
    "    print(self.model.summary())\n",
    "\n",
    "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "    if epoch % self.frequency == 0:\n",
    "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "  def log_train_opt(self, name):\n",
    "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "    print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "  def log_train_end(self, epoch, custom=\"\"):\n",
    "    print(\"==================\")\n",
    "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7Azv7DZp0M-"
   },
   "source": [
    "custom_lbfgs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjvMe1Avpvh9",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "  global global_time_list, global_last_time\n",
    "  global_time_list = []\n",
    "  global_last_time = time.perf_counter()\n",
    "  \n",
    "def record_time():\n",
    "  global global_last_time, global_time_list\n",
    "  new_time = time.perf_counter()\n",
    "  global_time_list.append(new_time - global_last_time)\n",
    "  global_last_time = time.perf_counter()\n",
    "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "  \"\"\"Returns last interval records in millis.\"\"\"\n",
    "  global global_last_time, global_time_list\n",
    "  if global_time_list:\n",
    "    return 1000 * global_time_list[-1]\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def dot(a, b):\n",
    "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "  return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "  print(s)\n",
    "\n",
    "final_loss = None\n",
    "times = []\n",
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "  \"\"\"\n",
    "\n",
    "  if config.maxIter == 0:\n",
    "    return\n",
    "\n",
    "  global final_loss, times\n",
    "  \n",
    "  maxIter = config.maxIter\n",
    "  maxEval = config.maxEval or maxIter*1.25\n",
    "  tolFun = config.tolFun or 1e-5\n",
    "  tolX = config.tolX or 1e-19\n",
    "  nCorrection = config.nCorrection or 100\n",
    "  lineSearch = config.lineSearch\n",
    "  lineSearchOpts = config.lineSearchOptions\n",
    "  learningRate = config.learningRate or 1\n",
    "  isverbose = config.verbose or False\n",
    "\n",
    "  # verbose function\n",
    "  if isverbose:\n",
    "    verbose = verbose_func\n",
    "  else:\n",
    "    verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "  f, g = opfunc(x)\n",
    "\n",
    "  f_hist = [f]\n",
    "  currentFuncEval = 1\n",
    "  state.funcEval = state.funcEval + 1\n",
    "  p = g.shape[0]\n",
    "\n",
    "  # check optimality of initial point\n",
    "  tmp1 = tf.abs(g)\n",
    "  if tf.reduce_sum(tmp1) <= tolFun:\n",
    "    verbose(\"optimality condition below tolFun\")\n",
    "    return x, f_hist\n",
    "\n",
    "  # optimize for a max of maxIter iterations\n",
    "  nIter = 0\n",
    "  times = []\n",
    "  while nIter < maxIter:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # keep track of nb of iterations\n",
    "    nIter = nIter + 1\n",
    "    state.nIter = state.nIter + 1\n",
    "\n",
    "    ############################################################\n",
    "    ## compute gradient descent direction\n",
    "    ############################################################\n",
    "    if state.nIter == 1:\n",
    "      d = -g\n",
    "      old_dirs = []\n",
    "      old_stps = []\n",
    "      Hdiag = 1\n",
    "    else:\n",
    "      # do lbfgs update (update memory)\n",
    "      y = g - g_old\n",
    "      s = d*t\n",
    "      ys = dot(y, s)\n",
    "      \n",
    "      if ys > 1e-10:\n",
    "        # updating memory\n",
    "        if len(old_dirs) == nCorrection:\n",
    "          # shift history by one (limited-memory)\n",
    "          del old_dirs[0]\n",
    "          del old_stps[0]\n",
    "\n",
    "        # store new direction/step\n",
    "        old_dirs.append(s)\n",
    "        old_stps.append(y)\n",
    "\n",
    "        # update scale of initial Hessian approximation\n",
    "        Hdiag = ys/dot(y, y)\n",
    "\n",
    "      # compute the approximate (L-BFGS) inverse Hessian \n",
    "      # multiplied by the gradient\n",
    "      k = len(old_dirs)\n",
    "\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      ro = [0]*nCorrection\n",
    "      for i in range(k):\n",
    "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "        \n",
    "\n",
    "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      al = [0]*nCorrection\n",
    "\n",
    "      q = -g\n",
    "      for i in range(k-1, -1, -1):\n",
    "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "        q = q - al[i]*old_stps[i]\n",
    "\n",
    "      # multiply by initial Hessian\n",
    "      r = q*Hdiag\n",
    "      for i in range(k):\n",
    "        be_i = dot(old_stps[i], r) * ro[i]\n",
    "        r += (al[i]-be_i)*old_dirs[i]\n",
    "        \n",
    "      d = r\n",
    "      # final direction is in r/d (same object)\n",
    "\n",
    "    g_old = g\n",
    "    f_old = f\n",
    "    \n",
    "    ############################################################\n",
    "    ## compute step length\n",
    "    ############################################################\n",
    "    # directional derivative\n",
    "    gtd = dot(g, d)\n",
    "\n",
    "    # check that progress can be made along that direction\n",
    "    if gtd > -tolX:\n",
    "      verbose(\"Can not make progress along direction.\")\n",
    "      break\n",
    "\n",
    "    # reset initial guess for step size\n",
    "    if state.nIter == 1:\n",
    "      tmp1 = tf.abs(g)\n",
    "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "    else:\n",
    "      t = learningRate\n",
    "\n",
    "\n",
    "    # optional line search: user function\n",
    "    lsFuncEval = 0\n",
    "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "      # perform line search, using user function\n",
    "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "      f_hist.append(f)\n",
    "    else:\n",
    "      # no line search, simply move with fixed-step\n",
    "      x += t*d\n",
    "      \n",
    "      if nIter != maxIter:\n",
    "        # re-evaluate function only if not in last iteration\n",
    "        # the reason we do this: in a stochastic setting,\n",
    "        # no use to re-evaluate that function here\n",
    "        f, g = opfunc(x)\n",
    "        lsFuncEval = 1\n",
    "        f_hist.append(f)\n",
    "\n",
    "\n",
    "    # update func eval\n",
    "    currentFuncEval = currentFuncEval + lsFuncEval\n",
    "    state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "    ############################################################\n",
    "    ## check conditions\n",
    "    ############################################################\n",
    "    if nIter == maxIter:\n",
    "      break\n",
    "\n",
    "    if currentFuncEval >= maxEval:\n",
    "      # max nb of function evals\n",
    "      verbose('max nb of function evals')\n",
    "      break\n",
    "\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <=tolFun:\n",
    "      # check optimality\n",
    "      verbose('optimality condition below tolFun')\n",
    "      break\n",
    "    \n",
    "    tmp1 = tf.abs(d*t)\n",
    "    if tf.reduce_sum(tmp1) <= tolX:\n",
    "      # step size below tolX\n",
    "      verbose('step size below tolX')\n",
    "      break\n",
    "\n",
    "    if tf.abs(f-f_old) < tolX:\n",
    "      # function value changing less than tolX\n",
    "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "      break\n",
    "\n",
    "    if do_verbose:\n",
    "      log_fn(nIter, f.numpy(), True)\n",
    "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "      record_time()\n",
    "      times.append(last_time())\n",
    "\n",
    "    if nIter == maxIter - 1:\n",
    "      final_loss = f.numpy()\n",
    "\n",
    "\n",
    "  # save state\n",
    "  state.old_dirs = old_dirs\n",
    "  state.old_stps = old_stps\n",
    "  state.Hdiag = Hdiag\n",
    "  state.g_old = g_old\n",
    "  state.f_old = f_old\n",
    "  state.t = t\n",
    "  state.d = d\n",
    "\n",
    "  return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "  pass\n",
    "\n",
    "class Struct(dummy):\n",
    "  def __getattribute__(self, key):\n",
    "    if key == '__dict__':\n",
    "      return super(dummy, self).__getattribute__('__dict__')\n",
    "    return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOT-E8C4oAJN"
   },
   "source": [
    "# 1. Continuous Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Qrt3ECzcLHp"
   },
   "source": [
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8CHqrpafela"
   },
   "source": [
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sDOc1flUyUvp",
    "outputId": "fe6001c4-69f0-4907-de79-88d68a69ddd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "x = loadmat(os.path.join(appDataPath, \"burgers_shock.mat\"))\n",
    "x[\"usol\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9Ko6L87J2v_"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwWhiecUqbAo",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (1-sized input [t], 8 hidden layer of 20-width, 1-sized output [theta]\n",
    "layers = [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 200\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAAb6gDT2zKK"
   },
   "outputs": [],
   "source": [
    "# # # Getting the data\n",
    "# path = 'pendulum_with_time_data.npz'\n",
    "# # # x is an array of shape (256,1) with values ranging from -1 to 1\n",
    "# # # t is an array of shape (100,1) with values ranging from 0 to 1 \n",
    "# # # usol is 2D array of shape (256,100) with data from the burger's equation\n",
    "# # # X and T are both shapes of (256,100) created by np.meshgrid\n",
    "# # # Exact_u has shape of (100, 256) from transposing usol\n",
    "# # # X_star hss shape of (25600, 2) created by flattening both X and T. Each array is (X,T)\n",
    "# # # u_star has shape of (25600, 1) created by flattening  Exact_u\n",
    "# # # X_u_train has shape of (N_u, 2) created by randomly choosing indices from X_star\n",
    "# # # X_f = lb + (ub-lb)*lhs(2, N_f)  lhs=latin hypercube\n",
    "# # # ub is upper bound\n",
    "# # # lb is lower bound\n",
    "# t, theta, t_train, theta_train, t_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "# theta_train[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkimJNtepkKi"
   },
   "source": [
    "## PINN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3wUjV9oe7V9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVm9UCvvlyY_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, t_f, ub, lb, g=10.0, l=1.):\n",
    "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "    self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.g = g\n",
    "    self.l = l\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Separating the collocation coordinates\n",
    "    self.t_f = tf.convert_to_tensor(t_f[:, None], dtype=self.dtype)\n",
    "    # self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "    \n",
    "  # Defining custom loss\n",
    "  def __loss(self, theta_train, u_pred):\n",
    "    f_pred = self.f_model()\n",
    "    return tf.reduce_mean(tf.square(theta_train - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, t_train, theta_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(theta_train, self.u_model(t_train))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  # The actual PINN\n",
    "  def f_model(self):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(self.t_f)\n",
    "      # Packing together the inputs\n",
    "      # t_f = tf.stack([self.t_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(self.t_f)\n",
    "      # Deriving INSIDE the tape\n",
    "      u_t = tape.gradient(u, self.t_f)\n",
    "    \n",
    "    # Getting the second derivative\n",
    "    u_tt = tape.gradient(u_t, self.t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    g, l = self.get_params(numpy=True)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_tt + (g/l)*np.sin(u) \n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.g, self.l\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.u_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, t_train, theta_train, tf_epochs=5000, nt_config=Struct()):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    t_train = tf.convert_to_tensor(t_train[:,None], dtype=self.dtype)\n",
    "    theta_train = tf.convert_to_tensor(theta_train[:,None], dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(t_train, theta_train)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(theta_train, self.u_model(t_train))\n",
    "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "  def predict(self, t):\n",
    "    theta_pred = self.u_model(t)\n",
    "    f_pred = self.f_model()\n",
    "    return theta_pred, f_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FzMd65dpoHo"
   },
   "source": [
    "## Training and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SEKkpHvApf46",
    "lines_to_next_cell": 2,
    "outputId": "276cad86-e9d4-412f-a67f-017c1a73b34b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "Eager execution: True\n",
      "WARNING:tensorflow:From <ipython-input-7-cc6fc44b5ad3>:151: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf_epoch =      0  elapsed = 00:02  loss = 2.5112e+00  error = 2.8028e+01  \n",
      "tf_epoch =     10  elapsed = 00:03  loss = 4.3345e+01  error = 2.6573e+01  \n",
      "tf_epoch =     20  elapsed = 00:03  loss = 7.4804e+00  error = 2.8006e+01  \n",
      "tf_epoch =     30  elapsed = 00:04  loss = 3.6162e+01  error = 2.6524e+01  \n",
      "tf_epoch =     40  elapsed = 00:05  loss = 2.9021e+01  error = 2.6558e+01  \n",
      "tf_epoch =     50  elapsed = 00:06  loss = 1.0324e+01  error = 2.7708e+01  \n",
      "tf_epoch =     60  elapsed = 00:06  loss = 2.9564e+01  error = 2.6481e+01  \n",
      "tf_epoch =     70  elapsed = 00:07  loss = 3.4043e+01  error = 2.6479e+01  \n",
      "tf_epoch =     80  elapsed = 00:08  loss = 1.6964e+01  error = 2.7150e+01  \n",
      "tf_epoch =     90  elapsed = 00:09  loss = 1.4155e+01  error = 2.7124e+01  \n",
      "tf_epoch =    100  elapsed = 00:09  loss = 3.0286e+01  error = 2.6481e+01  \n",
      "tf_epoch =    110  elapsed = 00:10  loss = 2.7819e+01  error = 2.6553e+01  \n",
      "tf_epoch =    120  elapsed = 00:11  loss = 1.3660e+01  error = 2.7458e+01  \n",
      "tf_epoch =    130  elapsed = 00:11  loss = 2.2429e+01  error = 2.6630e+01  \n",
      "tf_epoch =    140  elapsed = 00:12  loss = 2.8071e+01  error = 2.6539e+01  \n",
      "tf_epoch =    150  elapsed = 00:13  loss = 1.5172e+01  error = 2.7519e+01  \n",
      "tf_epoch =    160  elapsed = 00:14  loss = 2.4001e+01  error = 2.6650e+01  \n",
      "tf_epoch =    170  elapsed = 00:14  loss = 2.3420e+01  error = 2.6965e+01  \n",
      "tf_epoch =    180  elapsed = 00:15  loss = 2.3843e+01  error = 2.7755e+01  \n",
      "tf_epoch =    190  elapsed = 00:16  loss = 2.3034e+01  error = 2.7446e+01  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:17  loss = 2.1605e+01  error = 2.7438e+01  \n",
      "nt_epoch =     20  elapsed = 00:18  loss = 2.0824e+01  error = 2.7332e+01  \n",
      "nt_epoch =     30  elapsed = 00:19  loss = 2.0880e+01  error = 2.7421e+01  \n",
      "nt_epoch =     40  elapsed = 00:20  loss = 2.1029e+01  error = 2.7202e+01  \n",
      "nt_epoch =     50  elapsed = 00:21  loss = 2.1119e+01  error = 2.7189e+01  \n",
      "nt_epoch =     60  elapsed = 00:23  loss = 2.0787e+01  error = 2.7334e+01  \n",
      "nt_epoch =     70  elapsed = 00:24  loss = 2.1797e+01  error = 2.7282e+01  \n",
      "nt_epoch =     80  elapsed = 00:25  loss = 2.0884e+01  error = 2.7338e+01  \n",
      "nt_epoch =     90  elapsed = 00:26  loss = 2.1420e+01  error = 2.7298e+01  \n",
      "nt_epoch =    100  elapsed = 00:27  loss = 2.1274e+01  error = 2.7286e+01  \n",
      "nt_epoch =    110  elapsed = 00:28  loss = 2.1340e+01  error = 2.7178e+01  \n",
      "nt_epoch =    120  elapsed = 00:29  loss = 2.0848e+01  error = 2.7313e+01  \n",
      "nt_epoch =    130  elapsed = 00:31  loss = 2.2876e+01  error = 2.7201e+01  \n",
      "nt_epoch =    140  elapsed = 00:32  loss = 2.2331e+01  error = 2.7362e+01  \n",
      "nt_epoch =    150  elapsed = 00:33  loss = 2.1056e+01  error = 2.7683e+01  \n",
      "nt_epoch =    160  elapsed = 00:34  loss = 2.1197e+01  error = 2.7446e+01  \n",
      "nt_epoch =    170  elapsed = 00:35  loss = 2.0193e+01  error = 2.7528e+01  \n",
      "nt_epoch =    180  elapsed = 00:36  loss = 2.0546e+01  error = 2.7501e+01  \n",
      "nt_epoch =    190  elapsed = 00:37  loss = 2.0397e+01  error = 2.7555e+01  \n",
      "nt_epoch =    200  elapsed = 00:38  loss = 2.0012e+01  error = 2.7556e+01  \n",
      "nt_epoch =    210  elapsed = 00:39  loss = 2.0318e+01  error = 2.7596e+01  \n",
      "nt_epoch =    220  elapsed = 00:41  loss = 1.9971e+01  error = 2.7597e+01  \n",
      "nt_epoch =    230  elapsed = 00:42  loss = 1.9850e+01  error = 2.7628e+01  \n",
      "nt_epoch =    240  elapsed = 00:43  loss = 1.9844e+01  error = 2.7626e+01  \n",
      "nt_epoch =    250  elapsed = 00:44  loss = 1.9783e+01  error = 2.7577e+01  \n",
      "nt_epoch =    260  elapsed = 00:45  loss = 1.9794e+01  error = 2.7579e+01  \n",
      "nt_epoch =    270  elapsed = 00:46  loss = 1.9788e+01  error = 2.7562e+01  \n",
      "nt_epoch =    280  elapsed = 00:47  loss = 1.9784e+01  error = 2.7605e+01  \n",
      "nt_epoch =    290  elapsed = 00:49  loss = 1.9874e+01  error = 2.7567e+01  \n",
      "nt_epoch =    300  elapsed = 00:50  loss = 1.9846e+01  error = 2.7557e+01  \n",
      "nt_epoch =    310  elapsed = 00:51  loss = 1.9859e+01  error = 2.7529e+01  \n",
      "nt_epoch =    320  elapsed = 00:52  loss = 1.9825e+01  error = 2.7537e+01  \n",
      "nt_epoch =    330  elapsed = 00:53  loss = 1.9934e+01  error = 2.7532e+01  \n",
      "nt_epoch =    340  elapsed = 00:54  loss = 1.9828e+01  error = 2.7565e+01  \n",
      "nt_epoch =    350  elapsed = 00:55  loss = 1.9890e+01  error = 2.7561e+01  \n",
      "nt_epoch =    360  elapsed = 00:56  loss = 1.9830e+01  error = 2.7575e+01  \n",
      "nt_epoch =    370  elapsed = 00:58  loss = 1.9863e+01  error = 2.7577e+01  \n",
      "nt_epoch =    380  elapsed = 00:59  loss = 1.9960e+01  error = 2.7560e+01  \n",
      "nt_epoch =    390  elapsed = 01:00  loss = 1.9985e+01  error = 2.7546e+01  \n",
      "nt_epoch =    400  elapsed = 01:01  loss = 2.0012e+01  error = 2.7540e+01  \n",
      "nt_epoch =    410  elapsed = 01:02  loss = 2.0008e+01  error = 2.7548e+01  \n",
      "nt_epoch =    420  elapsed = 01:03  loss = 2.0049e+01  error = 2.7581e+01  \n",
      "nt_epoch =    430  elapsed = 01:04  loss = 2.0082e+01  error = 2.7594e+01  \n",
      "nt_epoch =    440  elapsed = 01:05  loss = 2.0013e+01  error = 2.7661e+01  \n",
      "nt_epoch =    450  elapsed = 01:06  loss = 2.0109e+01  error = 2.7655e+01  \n",
      "nt_epoch =    460  elapsed = 01:08  loss = 2.0069e+01  error = 2.7639e+01  \n",
      "nt_epoch =    470  elapsed = 01:09  loss = 2.0062e+01  error = 2.7630e+01  \n",
      "nt_epoch =    480  elapsed = 01:10  loss = 2.0093e+01  error = 2.7602e+01  \n",
      "nt_epoch =    490  elapsed = 01:11  loss = 2.0232e+01  error = 2.7631e+01  \n",
      "nt_epoch =    500  elapsed = 01:12  loss = 2.0119e+01  error = 2.7620e+01  \n",
      "nt_epoch =    510  elapsed = 01:13  loss = 2.0156e+01  error = 2.7606e+01  \n",
      "nt_epoch =    520  elapsed = 01:14  loss = 2.0240e+01  error = 2.7612e+01  \n",
      "nt_epoch =    530  elapsed = 01:16  loss = 2.0242e+01  error = 2.7564e+01  \n",
      "nt_epoch =    540  elapsed = 01:17  loss = 2.0259e+01  error = 2.7526e+01  \n",
      "nt_epoch =    550  elapsed = 01:18  loss = 2.0272e+01  error = 2.7520e+01  \n",
      "nt_epoch =    560  elapsed = 01:19  loss = 2.0138e+01  error = 2.7526e+01  \n",
      "nt_epoch =    570  elapsed = 01:20  loss = 2.0219e+01  error = 2.7539e+01  \n",
      "nt_epoch =    580  elapsed = 01:21  loss = 2.0212e+01  error = 2.7544e+01  \n",
      "nt_epoch =    590  elapsed = 01:22  loss = 2.0212e+01  error = 2.7536e+01  \n",
      "nt_epoch =    600  elapsed = 01:23  loss = 2.0240e+01  error = 2.7535e+01  \n",
      "nt_epoch =    610  elapsed = 01:25  loss = 2.0199e+01  error = 2.7544e+01  \n",
      "nt_epoch =    620  elapsed = 01:26  loss = 2.0183e+01  error = 2.7546e+01  \n",
      "nt_epoch =    630  elapsed = 01:27  loss = 2.0184e+01  error = 2.7546e+01  \n",
      "nt_epoch =    640  elapsed = 01:28  loss = 2.0192e+01  error = 2.7550e+01  \n",
      "nt_epoch =    650  elapsed = 01:29  loss = 2.0212e+01  error = 2.7549e+01  \n",
      "nt_epoch =    660  elapsed = 01:30  loss = 2.0217e+01  error = 2.7548e+01  \n",
      "nt_epoch =    670  elapsed = 01:31  loss = 2.0218e+01  error = 2.7536e+01  \n",
      "nt_epoch =    680  elapsed = 01:32  loss = 2.0218e+01  error = 2.7536e+01  \n",
      "nt_epoch =    690  elapsed = 01:34  loss = 2.0214e+01  error = 2.7524e+01  \n",
      "nt_epoch =    700  elapsed = 01:35  loss = 2.0217e+01  error = 2.7523e+01  \n",
      "nt_epoch =    710  elapsed = 01:36  loss = 2.0193e+01  error = 2.7536e+01  \n",
      "nt_epoch =    720  elapsed = 01:37  loss = 2.0207e+01  error = 2.7530e+01  \n",
      "nt_epoch =    730  elapsed = 01:38  loss = 2.0213e+01  error = 2.7533e+01  \n",
      "nt_epoch =    740  elapsed = 01:39  loss = 2.0242e+01  error = 2.7527e+01  \n",
      "nt_epoch =    750  elapsed = 01:40  loss = 2.0202e+01  error = 2.7540e+01  \n",
      "nt_epoch =    760  elapsed = 01:42  loss = 2.0224e+01  error = 2.7533e+01  \n",
      "nt_epoch =    770  elapsed = 01:43  loss = 2.0228e+01  error = 2.7534e+01  \n",
      "nt_epoch =    780  elapsed = 01:44  loss = 2.0240e+01  error = 2.7539e+01  \n",
      "nt_epoch =    790  elapsed = 01:45  loss = 2.0231e+01  error = 2.7538e+01  \n",
      "nt_epoch =    800  elapsed = 01:46  loss = 2.0250e+01  error = 2.7531e+01  \n",
      "nt_epoch =    810  elapsed = 01:47  loss = 2.0243e+01  error = 2.7531e+01  \n",
      "nt_epoch =    820  elapsed = 01:48  loss = 2.0240e+01  error = 2.7536e+01  \n",
      "nt_epoch =    830  elapsed = 01:49  loss = 2.0247e+01  error = 2.7535e+01  \n",
      "nt_epoch =    840  elapsed = 01:51  loss = 2.0237e+01  error = 2.7534e+01  \n",
      "nt_epoch =    850  elapsed = 01:52  loss = 2.0271e+01  error = 2.7525e+01  \n",
      "nt_epoch =    860  elapsed = 01:53  loss = 2.0269e+01  error = 2.7525e+01  \n",
      "nt_epoch =    870  elapsed = 01:54  loss = 2.0257e+01  error = 2.7527e+01  \n",
      "nt_epoch =    880  elapsed = 01:55  loss = 2.0249e+01  error = 2.7531e+01  \n",
      "nt_epoch =    890  elapsed = 01:56  loss = 2.0245e+01  error = 2.7534e+01  \n",
      "nt_epoch =    900  elapsed = 01:57  loss = 2.0232e+01  error = 2.7533e+01  \n",
      "nt_epoch =    910  elapsed = 01:58  loss = 2.0212e+01  error = 2.7537e+01  \n",
      "nt_epoch =    920  elapsed = 02:00  loss = 2.0238e+01  error = 2.7533e+01  \n",
      "nt_epoch =    930  elapsed = 02:01  loss = 2.0234e+01  error = 2.7534e+01  \n",
      "nt_epoch =    940  elapsed = 02:02  loss = 2.0234e+01  error = 2.7538e+01  \n",
      "nt_epoch =    950  elapsed = 02:03  loss = 2.0239e+01  error = 2.7534e+01  \n",
      "nt_epoch =    960  elapsed = 02:04  loss = 2.0238e+01  error = 2.7533e+01  \n",
      "nt_epoch =    970  elapsed = 02:05  loss = 2.0247e+01  error = 2.7532e+01  \n",
      "nt_epoch =    980  elapsed = 02:06  loss = 2.0231e+01  error = 2.7534e+01  \n",
      "nt_epoch =    990  elapsed = 02:07  loss = 2.0220e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1000  elapsed = 02:09  loss = 2.0215e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1010  elapsed = 02:10  loss = 2.0238e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1020  elapsed = 02:11  loss = 2.0242e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1030  elapsed = 02:12  loss = 2.0236e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1040  elapsed = 02:13  loss = 2.0233e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1050  elapsed = 02:14  loss = 2.0228e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1060  elapsed = 02:15  loss = 2.0231e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1070  elapsed = 02:17  loss = 2.0230e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1080  elapsed = 02:18  loss = 2.0232e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1090  elapsed = 02:19  loss = 2.0217e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1100  elapsed = 02:20  loss = 2.0230e+01  error = 2.7539e+01  \n",
      "nt_epoch =   1110  elapsed = 02:21  loss = 2.0236e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1120  elapsed = 02:22  loss = 2.0222e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1130  elapsed = 02:23  loss = 2.0221e+01  error = 2.7545e+01  \n",
      "nt_epoch =   1140  elapsed = 02:24  loss = 2.0227e+01  error = 2.7544e+01  \n",
      "nt_epoch =   1150  elapsed = 02:26  loss = 2.0238e+01  error = 2.7546e+01  \n",
      "nt_epoch =   1160  elapsed = 02:27  loss = 2.0245e+01  error = 2.7544e+01  \n",
      "nt_epoch =   1170  elapsed = 02:28  loss = 2.0239e+01  error = 2.7545e+01  \n",
      "nt_epoch =   1180  elapsed = 02:29  loss = 2.0237e+01  error = 2.7544e+01  \n",
      "nt_epoch =   1190  elapsed = 02:30  loss = 2.0240e+01  error = 2.7543e+01  \n",
      "nt_epoch =   1200  elapsed = 02:31  loss = 2.0234e+01  error = 2.7544e+01  \n",
      "nt_epoch =   1210  elapsed = 02:32  loss = 2.0239e+01  error = 2.7541e+01  \n",
      "nt_epoch =   1220  elapsed = 02:33  loss = 2.0242e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1230  elapsed = 02:35  loss = 2.0243e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1240  elapsed = 02:36  loss = 2.0242e+01  error = 2.7541e+01  \n",
      "nt_epoch =   1250  elapsed = 02:37  loss = 2.0249e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1260  elapsed = 02:38  loss = 2.0240e+01  error = 2.7543e+01  \n",
      "nt_epoch =   1270  elapsed = 02:39  loss = 2.0246e+01  error = 2.7541e+01  \n",
      "nt_epoch =   1280  elapsed = 02:40  loss = 2.0252e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1290  elapsed = 02:41  loss = 2.0253e+01  error = 2.7539e+01  \n",
      "nt_epoch =   1300  elapsed = 02:42  loss = 2.0253e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1310  elapsed = 02:44  loss = 2.0247e+01  error = 2.7543e+01  \n",
      "nt_epoch =   1320  elapsed = 02:45  loss = 2.0251e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1330  elapsed = 02:46  loss = 2.0252e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1340  elapsed = 02:47  loss = 2.0256e+01  error = 2.7544e+01  \n",
      "nt_epoch =   1350  elapsed = 02:48  loss = 2.0260e+01  error = 2.7543e+01  \n",
      "nt_epoch =   1360  elapsed = 02:49  loss = 2.0259e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1370  elapsed = 02:50  loss = 2.0260e+01  error = 2.7543e+01  \n",
      "nt_epoch =   1380  elapsed = 02:51  loss = 2.0260e+01  error = 2.7542e+01  \n",
      "nt_epoch =   1390  elapsed = 02:53  loss = 2.0265e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1400  elapsed = 02:54  loss = 2.0261e+01  error = 2.7540e+01  \n",
      "nt_epoch =   1410  elapsed = 02:55  loss = 2.0266e+01  error = 2.7539e+01  \n",
      "nt_epoch =   1420  elapsed = 02:56  loss = 2.0265e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1430  elapsed = 02:57  loss = 2.0264e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1440  elapsed = 02:58  loss = 2.0268e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1450  elapsed = 02:59  loss = 2.0268e+01  error = 2.7531e+01  \n",
      "nt_epoch =   1460  elapsed = 03:01  loss = 2.0266e+01  error = 2.7532e+01  \n",
      "nt_epoch =   1470  elapsed = 03:02  loss = 2.0267e+01  error = 2.7531e+01  \n",
      "nt_epoch =   1480  elapsed = 03:03  loss = 2.0266e+01  error = 2.7532e+01  \n",
      "nt_epoch =   1490  elapsed = 03:04  loss = 2.0267e+01  error = 2.7531e+01  \n",
      "nt_epoch =   1500  elapsed = 03:05  loss = 2.0265e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1510  elapsed = 03:06  loss = 2.0268e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1520  elapsed = 03:07  loss = 2.0271e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1530  elapsed = 03:09  loss = 2.0266e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1540  elapsed = 03:10  loss = 2.0262e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1550  elapsed = 03:11  loss = 2.0256e+01  error = 2.7539e+01  \n",
      "nt_epoch =   1560  elapsed = 03:12  loss = 2.0259e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1570  elapsed = 03:13  loss = 2.0260e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1580  elapsed = 03:14  loss = 2.0250e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1590  elapsed = 03:15  loss = 2.0249e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1600  elapsed = 03:17  loss = 2.0257e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1610  elapsed = 03:18  loss = 2.0254e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1620  elapsed = 03:19  loss = 2.0243e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1630  elapsed = 03:20  loss = 2.0253e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1640  elapsed = 03:21  loss = 2.0245e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1650  elapsed = 03:22  loss = 2.0248e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1660  elapsed = 03:23  loss = 2.0247e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1670  elapsed = 03:24  loss = 2.0251e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1680  elapsed = 03:26  loss = 2.0253e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1690  elapsed = 03:27  loss = 2.0263e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1700  elapsed = 03:28  loss = 2.0259e+01  error = 2.7537e+01  \n",
      "nt_epoch =   1710  elapsed = 03:29  loss = 2.0264e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1720  elapsed = 03:30  loss = 2.0264e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1730  elapsed = 03:31  loss = 2.0261e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1740  elapsed = 03:32  loss = 2.0264e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1750  elapsed = 03:34  loss = 2.0269e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1760  elapsed = 03:35  loss = 2.0265e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1770  elapsed = 03:36  loss = 2.0263e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1780  elapsed = 03:37  loss = 2.0262e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1790  elapsed = 03:38  loss = 2.0263e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1800  elapsed = 03:39  loss = 2.0264e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1810  elapsed = 03:40  loss = 2.0261e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1820  elapsed = 03:42  loss = 2.0259e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1830  elapsed = 03:43  loss = 2.0255e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1840  elapsed = 03:44  loss = 2.0263e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1850  elapsed = 03:45  loss = 2.0261e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1860  elapsed = 03:46  loss = 2.0257e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1870  elapsed = 03:47  loss = 2.0258e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1880  elapsed = 03:48  loss = 2.0253e+01  error = 2.7534e+01  \n",
      "nt_epoch =   1890  elapsed = 03:49  loss = 2.0255e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1900  elapsed = 03:51  loss = 2.0260e+01  error = 2.7531e+01  \n",
      "nt_epoch =   1910  elapsed = 03:52  loss = 2.0252e+01  error = 2.7531e+01  \n",
      "nt_epoch =   1920  elapsed = 03:53  loss = 2.0253e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1930  elapsed = 03:54  loss = 2.0253e+01  error = 2.7533e+01  \n",
      "nt_epoch =   1940  elapsed = 03:55  loss = 2.0252e+01  error = 2.7535e+01  \n",
      "nt_epoch =   1950  elapsed = 03:56  loss = 2.0254e+01  error = 2.7536e+01  \n",
      "nt_epoch =   1960  elapsed = 03:57  loss = 2.0253e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1970  elapsed = 03:59  loss = 2.0254e+01  error = 2.7538e+01  \n",
      "nt_epoch =   1980  elapsed = 04:00  loss = 2.0253e+01  error = 2.7539e+01  \n",
      "nt_epoch =   1990  elapsed = 04:01  loss = 2.0251e+01  error = 2.7539e+01  \n",
      "==================\n",
      "Training finished (epoch 2200): duration = 04:02  error = 2.7539e+01  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Getting the data\n",
    "# x and t => replaced by one t\n",
    "# usol => replace by theta\n",
    "# X and T => unnecessary\n",
    "# Exact_u => unnecessary\n",
    "# X_star => t\n",
    "# u_star => theta\n",
    "# X_u_train => t_train\n",
    "# u_train => theta_train\n",
    "# X_f same\n",
    "# ub same\n",
    "# lb same\n",
    "path = 'pendulum_with_time_data.npz'\n",
    "t, theta, t_train, theta_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "# Variables needed: X_f, ub, lb, X_star, u_star, X_u_train, u_train\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, g=10.0, l=1.)\n",
    "def error():\n",
    "  theta_pred, _ = pinn.predict(t)\n",
    "  return np.linalg.norm(theta - theta_pred, 2) / np.linalg.norm(theta, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(t_train, theta_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same t that the predictions were previously gotten from\n",
    "theta_pred, f_pred = pinn.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wf1QXaK5PlUh",
    "lines_to_next_cell": 0,
    "outputId": "c2e79c95-4c1c-4c50-ced1-c3c0a410831b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.05,  0.1 ,  0.15,  0.2 ,  0.25,  0.3 ,  0.35,  0.4 ,\n",
       "        0.45,  0.5 ,  0.55,  0.6 ,  0.65,  0.7 ,  0.75,  0.8 ,  0.85,\n",
       "        0.9 ,  0.95,  1.  ,  1.05,  1.1 ,  1.15,  1.2 ,  1.25,  1.3 ,\n",
       "        1.35,  1.4 ,  1.45,  1.5 ,  1.55,  1.6 ,  1.65,  1.7 ,  1.75,\n",
       "        1.8 ,  1.85,  1.9 ,  1.95,  2.  ,  2.05,  2.1 ,  2.15,  2.2 ,\n",
       "        2.25,  2.3 ,  2.35,  2.4 ,  2.45,  2.5 ,  2.55,  2.6 ,  2.65,\n",
       "        2.7 ,  2.75,  2.8 ,  2.85,  2.9 ,  2.95,  3.  ,  3.05,  3.1 ,\n",
       "        3.15,  3.2 ,  3.25,  3.3 ,  3.35,  3.4 ,  3.45,  3.5 ,  3.55,\n",
       "        3.6 ,  3.65,  3.7 ,  3.75,  3.8 ,  3.85,  3.9 ,  3.95,  4.  ,\n",
       "        4.05,  4.1 ,  4.15,  4.2 ,  4.25,  4.3 ,  4.35,  4.4 ,  4.45,\n",
       "        4.5 ,  4.55,  4.6 ,  4.65,  4.7 ,  4.75,  4.8 ,  4.85,  4.9 ,\n",
       "        4.95,  5.  ,  5.05,  5.1 ,  5.15,  5.2 ,  5.25,  5.3 ,  5.35,\n",
       "        5.4 ,  5.45,  5.5 ,  5.55,  5.6 ,  5.65,  5.7 ,  5.75,  5.8 ,\n",
       "        5.85,  5.9 ,  5.95,  6.  ,  6.05,  6.1 ,  6.15,  6.2 ,  6.25,\n",
       "        6.3 ,  6.35,  6.4 ,  6.45,  6.5 ,  6.55,  6.6 ,  6.65,  6.7 ,\n",
       "        6.75,  6.8 ,  6.85,  6.9 ,  6.95,  7.  ,  7.05,  7.1 ,  7.15,\n",
       "        7.2 ,  7.25,  7.3 ,  7.35,  7.4 ,  7.45,  7.5 ,  7.55,  7.6 ,\n",
       "        7.65,  7.7 ,  7.75,  7.8 ,  7.85,  7.9 ,  7.95,  8.  ,  8.05,\n",
       "        8.1 ,  8.15,  8.2 ,  8.25,  8.3 ,  8.35,  8.4 ,  8.45,  8.5 ,\n",
       "        8.55,  8.6 ,  8.65,  8.7 ,  8.75,  8.8 ,  8.85,  8.9 ,  8.95,\n",
       "        9.  ,  9.05,  9.1 ,  9.15,  9.2 ,  9.25,  9.3 ,  9.35,  9.4 ,\n",
       "        9.45,  9.5 ,  9.55,  9.6 ,  9.65,  9.7 ,  9.75,  9.8 ,  9.85,\n",
       "        9.9 ,  9.95, 10.  , 10.05, 10.1 , 10.15, 10.2 , 10.25, 10.3 ,\n",
       "       10.35, 10.4 , 10.45, 10.5 , 10.55, 10.6 , 10.65, 10.7 , 10.75,\n",
       "       10.8 , 10.85, 10.9 , 10.95, 11.  , 11.05, 11.1 , 11.15, 11.2 ,\n",
       "       11.25, 11.3 , 11.35, 11.4 , 11.45, 11.5 , 11.55, 11.6 , 11.65,\n",
       "       11.7 , 11.75, 11.8 , 11.85, 11.9 , 11.95, 12.  , 12.05, 12.1 ,\n",
       "       12.15, 12.2 , 12.25, 12.3 , 12.35, 12.4 , 12.45, 12.5 , 12.55,\n",
       "       12.6 , 12.65, 12.7 , 12.75, 12.8 , 12.85, 12.9 , 12.95, 13.  ,\n",
       "       13.05, 13.1 , 13.15, 13.2 , 13.25, 13.3 , 13.35, 13.4 , 13.45,\n",
       "       13.5 , 13.55, 13.6 , 13.65, 13.7 , 13.75, 13.8 , 13.85, 13.9 ,\n",
       "       13.95, 14.  , 14.05, 14.1 , 14.15, 14.2 , 14.25, 14.3 , 14.35,\n",
       "       14.4 , 14.45, 14.5 , 14.55, 14.6 , 14.65, 14.7 , 14.75, 14.8 ,\n",
       "       14.85, 14.9 , 14.95, 15.  , 15.05, 15.1 , 15.15, 15.2 , 15.25,\n",
       "       15.3 , 15.35, 15.4 , 15.45, 15.5 , 15.55, 15.6 , 15.65, 15.7 ,\n",
       "       15.75, 15.8 , 15.85, 15.9 , 15.95, 16.  , 16.05, 16.1 , 16.15,\n",
       "       16.2 , 16.25, 16.3 , 16.35, 16.4 , 16.45, 16.5 , 16.55, 16.6 ,\n",
       "       16.65, 16.7 , 16.75, 16.8 , 16.85, 16.9 , 16.95, 17.  , 17.05,\n",
       "       17.1 , 17.15, 17.2 , 17.25, 17.3 , 17.35, 17.4 , 17.45, 17.5 ,\n",
       "       17.55, 17.6 , 17.65, 17.7 , 17.75, 17.8 , 17.85, 17.9 , 17.95,\n",
       "       18.  , 18.05, 18.1 , 18.15, 18.2 , 18.25, 18.3 , 18.35, 18.4 ,\n",
       "       18.45, 18.5 , 18.55, 18.6 , 18.65, 18.7 , 18.75, 18.8 , 18.85,\n",
       "       18.9 , 18.95, 19.  , 19.05, 19.1 , 19.15, 19.2 , 19.25, 19.3 ,\n",
       "       19.35, 19.4 , 19.45, 19.5 , 19.55, 19.6 , 19.65, 19.7 , 19.75,\n",
       "       19.8 , 19.85, 19.9 , 19.95, 20.  , 20.05, 20.1 , 20.15, 20.2 ,\n",
       "       20.25, 20.3 , 20.35, 20.4 , 20.45, 20.5 , 20.55, 20.6 , 20.65,\n",
       "       20.7 , 20.75, 20.8 , 20.85, 20.9 , 20.95, 21.  , 21.05, 21.1 ,\n",
       "       21.15, 21.2 , 21.25, 21.3 , 21.35, 21.4 , 21.45, 21.5 , 21.55,\n",
       "       21.6 , 21.65, 21.7 , 21.75, 21.8 , 21.85, 21.9 , 21.95, 22.  ,\n",
       "       22.05, 22.1 , 22.15, 22.2 , 22.25, 22.3 , 22.35, 22.4 , 22.45,\n",
       "       22.5 , 22.55, 22.6 , 22.65, 22.7 , 22.75, 22.8 , 22.85, 22.9 ,\n",
       "       22.95, 23.  , 23.05, 23.1 , 23.15, 23.2 , 23.25, 23.3 , 23.35,\n",
       "       23.4 , 23.45, 23.5 , 23.55, 23.6 , 23.65, 23.7 , 23.75, 23.8 ,\n",
       "       23.85, 23.9 , 23.95, 24.  , 24.05, 24.1 , 24.15, 24.2 , 24.25,\n",
       "       24.3 , 24.35, 24.4 , 24.45, 24.5 , 24.55, 24.6 , 24.65, 24.7 ,\n",
       "       24.75, 24.8 , 24.85, 24.9 , 24.95, 25.  , 25.05, 25.1 , 25.15,\n",
       "       25.2 , 25.25, 25.3 , 25.35, 25.4 , 25.45, 25.5 , 25.55, 25.6 ,\n",
       "       25.65, 25.7 , 25.75, 25.8 , 25.85, 25.9 , 25.95, 26.  , 26.05,\n",
       "       26.1 , 26.15, 26.2 , 26.25, 26.3 , 26.35, 26.4 , 26.45, 26.5 ,\n",
       "       26.55, 26.6 , 26.65, 26.7 , 26.75, 26.8 , 26.85, 26.9 , 26.95,\n",
       "       27.  , 27.05, 27.1 , 27.15, 27.2 , 27.25, 27.3 , 27.35, 27.4 ,\n",
       "       27.45, 27.5 , 27.55, 27.6 , 27.65, 27.7 , 27.75, 27.8 , 27.85,\n",
       "       27.9 , 27.95, 28.  , 28.05, 28.1 , 28.15, 28.2 , 28.25, 28.3 ,\n",
       "       28.35, 28.4 , 28.45, 28.5 , 28.55, 28.6 , 28.65, 28.7 , 28.75,\n",
       "       28.8 , 28.85, 28.9 , 28.95, 29.  , 29.05, 29.1 , 29.15, 29.2 ,\n",
       "       29.25, 29.3 , 29.35, 29.4 , 29.45, 29.5 , 29.55, 29.6 , 29.65,\n",
       "       29.7 , 29.75, 29.8 , 29.85, 29.9 , 29.95, 30.  , 30.05, 30.1 ,\n",
       "       30.15, 30.2 , 30.25, 30.3 , 30.35, 30.4 , 30.45, 30.5 , 30.55,\n",
       "       30.6 , 30.65, 30.7 , 30.75, 30.8 , 30.85, 30.9 , 30.95, 31.  ,\n",
       "       31.05, 31.1 , 31.15, 31.2 , 31.25, 31.3 , 31.35, 31.4 , 31.45,\n",
       "       31.5 , 31.55, 31.6 , 31.65, 31.7 , 31.75, 31.8 , 31.85, 31.9 ,\n",
       "       31.95, 32.  , 32.05, 32.1 , 32.15, 32.2 , 32.25, 32.3 , 32.35,\n",
       "       32.4 , 32.45, 32.5 , 32.55, 32.6 , 32.65, 32.7 , 32.75, 32.8 ,\n",
       "       32.85, 32.9 , 32.95, 33.  , 33.05, 33.1 , 33.15, 33.2 , 33.25,\n",
       "       33.3 , 33.35, 33.4 , 33.45, 33.5 , 33.55, 33.6 , 33.65, 33.7 ,\n",
       "       33.75, 33.8 , 33.85, 33.9 , 33.95, 34.  , 34.05, 34.1 , 34.15,\n",
       "       34.2 , 34.25, 34.3 , 34.35, 34.4 , 34.45, 34.5 , 34.55, 34.6 ,\n",
       "       34.65, 34.7 , 34.75, 34.8 , 34.85, 34.9 , 34.95, 35.  , 35.05,\n",
       "       35.1 , 35.15, 35.2 , 35.25, 35.3 , 35.35, 35.4 , 35.45, 35.5 ,\n",
       "       35.55, 35.6 , 35.65, 35.7 , 35.75, 35.8 , 35.85, 35.9 , 35.95,\n",
       "       36.  , 36.05, 36.1 , 36.15, 36.2 , 36.25, 36.3 , 36.35, 36.4 ,\n",
       "       36.45, 36.5 , 36.55, 36.6 , 36.65, 36.7 , 36.75, 36.8 , 36.85,\n",
       "       36.9 , 36.95, 37.  , 37.05, 37.1 , 37.15, 37.2 , 37.25, 37.3 ,\n",
       "       37.35, 37.4 , 37.45, 37.5 , 37.55, 37.6 , 37.65, 37.7 , 37.75,\n",
       "       37.8 , 37.85, 37.9 , 37.95, 38.  , 38.05, 38.1 , 38.15, 38.2 ,\n",
       "       38.25, 38.3 , 38.35, 38.4 , 38.45, 38.5 , 38.55, 38.6 , 38.65,\n",
       "       38.7 , 38.75, 38.8 , 38.85, 38.9 , 38.95, 39.  , 39.05, 39.1 ,\n",
       "       39.15, 39.2 , 39.25, 39.3 , 39.35, 39.4 , 39.45, 39.5 , 39.55,\n",
       "       39.6 , 39.65, 39.7 , 39.75, 39.8 , 39.85, 39.9 , 39.95, 40.  ,\n",
       "       40.05, 40.1 , 40.15, 40.2 , 40.25, 40.3 , 40.35, 40.4 , 40.45,\n",
       "       40.5 , 40.55, 40.6 , 40.65, 40.7 , 40.75, 40.8 , 40.85, 40.9 ,\n",
       "       40.95, 41.  , 41.05, 41.1 , 41.15, 41.2 , 41.25, 41.3 , 41.35,\n",
       "       41.4 , 41.45, 41.5 , 41.55, 41.6 , 41.65, 41.7 , 41.75, 41.8 ,\n",
       "       41.85, 41.9 , 41.95, 42.  , 42.05, 42.1 , 42.15, 42.2 , 42.25,\n",
       "       42.3 , 42.35, 42.4 , 42.45, 42.5 , 42.55, 42.6 , 42.65, 42.7 ,\n",
       "       42.75, 42.8 , 42.85, 42.9 , 42.95, 43.  , 43.05, 43.1 , 43.15,\n",
       "       43.2 , 43.25, 43.3 , 43.35, 43.4 , 43.45, 43.5 , 43.55, 43.6 ,\n",
       "       43.65, 43.7 , 43.75, 43.8 , 43.85, 43.9 , 43.95, 44.  , 44.05,\n",
       "       44.1 , 44.15, 44.2 , 44.25, 44.3 , 44.35, 44.4 , 44.45, 44.5 ,\n",
       "       44.55, 44.6 , 44.65, 44.7 , 44.75, 44.8 , 44.85, 44.9 , 44.95,\n",
       "       45.  , 45.05, 45.1 , 45.15, 45.2 , 45.25, 45.3 , 45.35, 45.4 ,\n",
       "       45.45, 45.5 , 45.55, 45.6 , 45.65, 45.7 , 45.75, 45.8 , 45.85,\n",
       "       45.9 , 45.95, 46.  , 46.05, 46.1 , 46.15, 46.2 , 46.25, 46.3 ,\n",
       "       46.35, 46.4 , 46.45, 46.5 , 46.55, 46.6 , 46.65, 46.7 , 46.75,\n",
       "       46.8 , 46.85, 46.9 , 46.95, 47.  , 47.05, 47.1 , 47.15, 47.2 ,\n",
       "       47.25, 47.3 , 47.35, 47.4 , 47.45, 47.5 , 47.55, 47.6 , 47.65,\n",
       "       47.7 , 47.75, 47.8 , 47.85, 47.9 , 47.95, 48.  , 48.05, 48.1 ,\n",
       "       48.15, 48.2 , 48.25, 48.3 , 48.35, 48.4 , 48.45, 48.5 , 48.55,\n",
       "       48.6 , 48.65, 48.7 , 48.75, 48.8 , 48.85, 48.9 , 48.95, 49.  ,\n",
       "       49.05, 49.1 , 49.15, 49.2 , 49.25, 49.3 , 49.35, 49.4 , 49.45,\n",
       "       49.5 , 49.55, 49.6 , 49.65, 49.7 , 49.75, 49.8 , 49.85, 49.9 ,\n",
       "       49.95])"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-Dt42ItxAqr"
   },
   "source": [
    "# 2. Discrete Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1oPFjYDxAqs"
   },
   "source": [
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4z7cQz_xAqt"
   },
   "source": [
    "Approximating $u(t,x)$ with a deep NN, we define the PINN: (TODO)\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss: (TODO)\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGd5zVtoxAqt"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKrkV1ChxAqu",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data size on initial condition on u\n",
    "N_n = 250\n",
    "# Number of RK stages\n",
    "q = 500\n",
    "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q+1-sized output [u_1^n(x), ..., u_{q+1}^n(x)]\n",
    "layers = [1, 50, 50, 50, q + 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 200\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  lr=0.001,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    "  epsilon=1e-08)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 1000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXtJ5GiaxAqw"
   },
   "source": [
    "## PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2acHqmorxAqx"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times):\n",
    "    self.lb = lb\n",
    "    self.ub = ub\n",
    "    self.nu = nu\n",
    "\n",
    "    self.dt = dt\n",
    "\n",
    "    self.q = max(q,1)\n",
    "    self.IRK_weights = IRK_weights\n",
    "    self.IRK_times = IRK_times\n",
    "\n",
    "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
    "    self.U_1_model = tf.keras.Sequential()\n",
    "    self.U_1_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.U_1_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.U_1_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    self.x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "  def U_0_model(self, x):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(x)\n",
    "      tape.watch(self.dummy_x0_tf)\n",
    "\n",
    "      # Getting the prediction, and removing the last item (q+1)\n",
    "      U_1 = self.U_1_model(x) # shape=(len(x), q+1)\n",
    "      U = U_1[:, :-1] # shape=(len(x), q)\n",
    "\n",
    "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
    "      g_U = tape.gradient(U, x, output_gradients=self.dummy_x0_tf)\n",
    "      U_x = tape.gradient(g_U, self.dummy_x0_tf)\n",
    "      g_U_x = tape.gradient(U_x, x, output_gradients=self.dummy_x0_tf)\n",
    "    \n",
    "    # Doing the last one outside the with, to optimize performance\n",
    "    # Impossible to do for the earlier grad, because they’re needed after\n",
    "    U_xx = tape.gradient(g_U_x, self.dummy_x0_tf)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
    "    nu = self.get_params(numpy=True)\n",
    "    N = U*U_x - nu*U_xx # shape=(len(x), q)\n",
    "    return U_1 + self.dt*tf.matmul(N, self.IRK_weights.T)\n",
    "\n",
    "  # Defining custom loss\n",
    "  def __loss(self, u_0, u_0_pred):\n",
    "    u_1_pred = self.U_1_model(self.x_1)\n",
    "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
    "      tf.reduce_sum(tf.square(u_1_pred))\n",
    "\n",
    "  def __grad(self, x_0, u_0):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.U_1_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.U_1_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.U_1_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.nu\n",
    "\n",
    "  def summary(self):\n",
    "    return self.U_1_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, x_0, u_0, tf_epochs, nt_config):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
    "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
    "\n",
    "    # Creating dummy tensors for the gradients\n",
    "    self.dummy_x0_tf = tf.ones([x_0.shape[0], self.q], dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(x_0, u_0)\n",
    "      self.optimizer.apply_gradients(\n",
    "        zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "\n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "      grad = tape.gradient(loss_value, self.U_1_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "    \n",
    "    self.logger.log_train_end(tf_epochs)\n",
    "\n",
    "  def predict(self, x_star):\n",
    "    u_star = self.U_1_model(x_star)[:, -1]\n",
    "    return u_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIq8U_a_xAqy"
   },
   "source": [
    "## Training and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d4ICbcZhxAqz",
    "outputId": "568ae687-0f36-4ec3-fe36-47a88caa1f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 501)               25551     \n",
      "=================================================================\n",
      "Total params: 30,751\n",
      "Trainable params: 30,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf_epoch =      0  elapsed = 00:00  loss = 6.1188e+04  error = 9.9183e-01  \n",
      "tf_epoch =     10  elapsed = 00:00  loss = 5.2999e+04  error = 9.1168e-01  \n",
      "tf_epoch =     20  elapsed = 00:00  loss = 4.0422e+04  error = 8.3942e-01  \n",
      "tf_epoch =     30  elapsed = 00:00  loss = 3.0798e+04  error = 1.0075e+00  \n",
      "tf_epoch =     40  elapsed = 00:01  loss = 2.6585e+04  error = 1.2057e+00  \n",
      "tf_epoch =     50  elapsed = 00:01  loss = 2.3989e+04  error = 1.3011e+00  \n",
      "tf_epoch =     60  elapsed = 00:01  loss = 2.1985e+04  error = 1.3448e+00  \n",
      "tf_epoch =     70  elapsed = 00:02  loss = 2.0261e+04  error = 1.3670e+00  \n",
      "tf_epoch =     80  elapsed = 00:02  loss = 1.8731e+04  error = 1.3788e+00  \n",
      "tf_epoch =     90  elapsed = 00:02  loss = 1.7442e+04  error = 1.3852e+00  \n",
      "tf_epoch =    100  elapsed = 00:02  loss = 1.6428e+04  error = 1.3889e+00  \n",
      "tf_epoch =    110  elapsed = 00:03  loss = 1.5687e+04  error = 1.3913e+00  \n",
      "tf_epoch =    120  elapsed = 00:03  loss = 1.5193e+04  error = 1.3929e+00  \n",
      "tf_epoch =    130  elapsed = 00:03  loss = 1.4885e+04  error = 1.3941e+00  \n",
      "tf_epoch =    140  elapsed = 00:04  loss = 1.4671e+04  error = 1.3951e+00  \n",
      "tf_epoch =    150  elapsed = 00:04  loss = 1.4462e+04  error = 1.3959e+00  \n",
      "tf_epoch =    160  elapsed = 00:04  loss = 1.4274e+04  error = 1.3966e+00  \n",
      "tf_epoch =    170  elapsed = 00:04  loss = 1.4054e+04  error = 1.3972e+00  \n",
      "tf_epoch =    180  elapsed = 00:05  loss = 1.3774e+04  error = 1.3978e+00  \n",
      "tf_epoch =    190  elapsed = 00:05  loss = 1.3375e+04  error = 1.3985e+00  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:06  loss = 1.0259e+04  error = 1.3989e+00  \n",
      "nt_epoch =     20  elapsed = 00:06  loss = 7.9947e+03  error = 1.3987e+00  \n",
      "nt_epoch =     30  elapsed = 00:07  loss = 6.6145e+03  error = 1.3957e+00  \n",
      "nt_epoch =     40  elapsed = 00:07  loss = 5.3140e+03  error = 1.3828e+00  \n",
      "nt_epoch =     50  elapsed = 00:08  loss = 4.1600e+03  error = 1.3290e+00  \n",
      "nt_epoch =     60  elapsed = 00:08  loss = 3.2127e+03  error = 4.9325e-01  \n",
      "nt_epoch =     70  elapsed = 00:09  loss = 2.4533e+03  error = 3.6252e-01  \n",
      "nt_epoch =     80  elapsed = 00:09  loss = 1.7228e+03  error = 2.5586e-01  \n",
      "nt_epoch =     90  elapsed = 00:10  loss = 1.3183e+03  error = 2.9766e-01  \n",
      "nt_epoch =    100  elapsed = 00:10  loss = 8.0438e+02  error = 2.6806e-01  \n",
      "nt_epoch =    110  elapsed = 00:11  loss = 5.4548e+02  error = 2.2995e-01  \n",
      "nt_epoch =    120  elapsed = 00:11  loss = 4.3997e+02  error = 1.7919e-01  \n",
      "nt_epoch =    130  elapsed = 00:12  loss = 3.7856e+02  error = 1.5562e-01  \n",
      "nt_epoch =    140  elapsed = 00:12  loss = 2.9236e+02  error = 1.4277e-01  \n",
      "nt_epoch =    150  elapsed = 00:13  loss = 1.8006e+02  error = 1.5280e-01  \n",
      "nt_epoch =    160  elapsed = 00:13  loss = 1.3496e+02  error = 1.4978e-01  \n",
      "nt_epoch =    170  elapsed = 00:14  loss = 1.1440e+02  error = 1.4094e-01  \n",
      "nt_epoch =    180  elapsed = 00:14  loss = 1.0087e+02  error = 1.2451e-01  \n",
      "nt_epoch =    190  elapsed = 00:15  loss = 8.9688e+01  error = 1.1183e-01  \n",
      "nt_epoch =    200  elapsed = 00:15  loss = 7.6578e+01  error = 1.0212e-01  \n",
      "nt_epoch =    210  elapsed = 00:16  loss = 6.5661e+01  error = 9.0840e-02  \n",
      "nt_epoch =    220  elapsed = 00:16  loss = 6.1009e+01  error = 8.4938e-02  \n",
      "nt_epoch =    230  elapsed = 00:17  loss = 5.4179e+01  error = 8.1157e-02  \n",
      "nt_epoch =    240  elapsed = 00:17  loss = 4.7268e+01  error = 7.1322e-02  \n",
      "nt_epoch =    250  elapsed = 00:18  loss = 4.3566e+01  error = 6.6101e-02  \n",
      "nt_epoch =    260  elapsed = 00:19  loss = 4.0057e+01  error = 6.3827e-02  \n",
      "nt_epoch =    270  elapsed = 00:19  loss = 3.6910e+01  error = 6.4613e-02  \n",
      "nt_epoch =    280  elapsed = 00:20  loss = 3.4212e+01  error = 6.1574e-02  \n",
      "nt_epoch =    290  elapsed = 00:20  loss = 3.2354e+01  error = 5.9140e-02  \n",
      "nt_epoch =    300  elapsed = 00:21  loss = 3.0135e+01  error = 5.9188e-02  \n",
      "nt_epoch =    310  elapsed = 00:21  loss = 2.8047e+01  error = 5.5916e-02  \n",
      "nt_epoch =    320  elapsed = 00:22  loss = 2.6851e+01  error = 5.7101e-02  \n",
      "nt_epoch =    330  elapsed = 00:22  loss = 2.5501e+01  error = 5.4026e-02  \n",
      "nt_epoch =    340  elapsed = 00:23  loss = 2.4338e+01  error = 5.2891e-02  \n",
      "nt_epoch =    350  elapsed = 00:23  loss = 2.3083e+01  error = 5.2442e-02  \n",
      "nt_epoch =    360  elapsed = 00:24  loss = 2.1646e+01  error = 5.0733e-02  \n",
      "nt_epoch =    370  elapsed = 00:24  loss = 2.0595e+01  error = 4.9290e-02  \n",
      "nt_epoch =    380  elapsed = 00:25  loss = 1.9626e+01  error = 4.7238e-02  \n",
      "nt_epoch =    390  elapsed = 00:25  loss = 1.8628e+01  error = 4.5533e-02  \n",
      "nt_epoch =    400  elapsed = 00:26  loss = 1.7701e+01  error = 4.2219e-02  \n",
      "nt_epoch =    410  elapsed = 00:26  loss = 1.6661e+01  error = 4.1654e-02  \n",
      "nt_epoch =    420  elapsed = 00:27  loss = 1.5623e+01  error = 3.8387e-02  \n",
      "nt_epoch =    430  elapsed = 00:28  loss = 1.4810e+01  error = 3.4844e-02  \n",
      "nt_epoch =    440  elapsed = 00:28  loss = 1.4449e+01  error = 3.4042e-02  \n",
      "nt_epoch =    450  elapsed = 00:29  loss = 1.3392e+01  error = 3.1836e-02  \n",
      "nt_epoch =    460  elapsed = 00:29  loss = 1.2741e+01  error = 3.0054e-02  \n",
      "nt_epoch =    470  elapsed = 00:30  loss = 1.2216e+01  error = 2.7591e-02  \n",
      "nt_epoch =    480  elapsed = 00:30  loss = 1.1911e+01  error = 2.6615e-02  \n",
      "nt_epoch =    490  elapsed = 00:31  loss = 1.1434e+01  error = 2.6114e-02  \n",
      "nt_epoch =    500  elapsed = 00:31  loss = 1.0923e+01  error = 2.4332e-02  \n",
      "nt_epoch =    510  elapsed = 00:32  loss = 1.0465e+01  error = 2.5426e-02  \n",
      "nt_epoch =    520  elapsed = 00:32  loss = 1.0106e+01  error = 2.4790e-02  \n",
      "nt_epoch =    530  elapsed = 00:33  loss = 9.7654e+00  error = 2.4459e-02  \n",
      "nt_epoch =    540  elapsed = 00:33  loss = 9.4612e+00  error = 2.4457e-02  \n",
      "nt_epoch =    550  elapsed = 00:34  loss = 9.0302e+00  error = 2.4392e-02  \n",
      "nt_epoch =    560  elapsed = 00:34  loss = 8.6807e+00  error = 2.5161e-02  \n",
      "nt_epoch =    570  elapsed = 00:35  loss = 8.3487e+00  error = 2.4390e-02  \n",
      "nt_epoch =    580  elapsed = 00:35  loss = 7.9939e+00  error = 2.4557e-02  \n",
      "nt_epoch =    590  elapsed = 00:36  loss = 7.6991e+00  error = 2.2813e-02  \n",
      "nt_epoch =    600  elapsed = 00:36  loss = 7.4683e+00  error = 2.2652e-02  \n",
      "nt_epoch =    610  elapsed = 00:37  loss = 7.3332e+00  error = 2.3342e-02  \n",
      "nt_epoch =    620  elapsed = 00:37  loss = 7.1412e+00  error = 2.2975e-02  \n",
      "nt_epoch =    630  elapsed = 00:38  loss = 6.8501e+00  error = 2.2818e-02  \n",
      "nt_epoch =    640  elapsed = 00:38  loss = 6.7133e+00  error = 2.3513e-02  \n",
      "nt_epoch =    650  elapsed = 00:39  loss = 6.5698e+00  error = 2.4563e-02  \n",
      "nt_epoch =    660  elapsed = 00:39  loss = 6.5088e+00  error = 2.4191e-02  \n",
      "nt_epoch =    670  elapsed = 00:40  loss = 6.3838e+00  error = 2.4255e-02  \n",
      "nt_epoch =    680  elapsed = 00:40  loss = 6.1974e+00  error = 2.4210e-02  \n",
      "nt_epoch =    690  elapsed = 00:41  loss = 6.0829e+00  error = 2.4437e-02  \n",
      "nt_epoch =    700  elapsed = 00:42  loss = 5.9533e+00  error = 2.3962e-02  \n",
      "nt_epoch =    710  elapsed = 00:42  loss = 5.8196e+00  error = 2.3958e-02  \n",
      "nt_epoch =    720  elapsed = 00:43  loss = 5.7019e+00  error = 2.4388e-02  \n",
      "nt_epoch =    730  elapsed = 00:43  loss = 5.5534e+00  error = 2.4159e-02  \n",
      "nt_epoch =    740  elapsed = 00:44  loss = 5.4217e+00  error = 2.4568e-02  \n",
      "nt_epoch =    750  elapsed = 00:44  loss = 5.2916e+00  error = 2.4499e-02  \n",
      "nt_epoch =    760  elapsed = 00:45  loss = 5.1786e+00  error = 2.4452e-02  \n",
      "nt_epoch =    770  elapsed = 00:45  loss = 5.1121e+00  error = 2.4085e-02  \n",
      "nt_epoch =    780  elapsed = 00:46  loss = 5.0397e+00  error = 2.3476e-02  \n",
      "nt_epoch =    790  elapsed = 00:46  loss = 4.9471e+00  error = 2.3345e-02  \n",
      "nt_epoch =    800  elapsed = 00:47  loss = 4.8737e+00  error = 2.3161e-02  \n",
      "nt_epoch =    810  elapsed = 00:47  loss = 4.7934e+00  error = 2.2827e-02  \n",
      "nt_epoch =    820  elapsed = 00:48  loss = 4.7183e+00  error = 2.3322e-02  \n",
      "nt_epoch =    830  elapsed = 00:48  loss = 4.6563e+00  error = 2.3498e-02  \n",
      "nt_epoch =    840  elapsed = 00:49  loss = 4.5554e+00  error = 2.4143e-02  \n",
      "nt_epoch =    850  elapsed = 00:49  loss = 4.4984e+00  error = 2.4137e-02  \n",
      "nt_epoch =    860  elapsed = 00:50  loss = 4.4153e+00  error = 2.4215e-02  \n",
      "nt_epoch =    870  elapsed = 00:50  loss = 4.3528e+00  error = 2.4172e-02  \n",
      "nt_epoch =    880  elapsed = 00:51  loss = 4.2765e+00  error = 2.3777e-02  \n",
      "nt_epoch =    890  elapsed = 00:51  loss = 4.2018e+00  error = 2.3575e-02  \n",
      "nt_epoch =    900  elapsed = 00:52  loss = 4.1105e+00  error = 2.3804e-02  \n",
      "nt_epoch =    910  elapsed = 00:52  loss = 4.0540e+00  error = 2.4106e-02  \n",
      "nt_epoch =    920  elapsed = 00:53  loss = 3.9929e+00  error = 2.3978e-02  \n",
      "nt_epoch =    930  elapsed = 00:53  loss = 3.9467e+00  error = 2.3623e-02  \n",
      "nt_epoch =    940  elapsed = 00:54  loss = 3.8917e+00  error = 2.3915e-02  \n",
      "nt_epoch =    950  elapsed = 00:54  loss = 3.8322e+00  error = 2.5110e-02  \n",
      "nt_epoch =    960  elapsed = 00:55  loss = 3.7762e+00  error = 2.4950e-02  \n",
      "nt_epoch =    970  elapsed = 00:55  loss = 3.7359e+00  error = 2.4904e-02  \n",
      "nt_epoch =    980  elapsed = 00:56  loss = 3.6824e+00  error = 2.5186e-02  \n",
      "nt_epoch =    990  elapsed = 00:56  loss = 3.6200e+00  error = 2.4975e-02  \n",
      "==================\n",
      "Training finished (epoch 200): duration = 00:57  error = 2.4198e-02  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "idx_t_0 = 10\n",
    "idx_t_1 = 90\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, dt, \\\n",
    "  Exact_u, x_0, u_0, x_1, x_star, u_star, \\\n",
    "  IRK_weights, IRK_times = prep_data(path, N_n=N_n, q=q, lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times)\n",
    "def error():\n",
    "  u_pred = pinn.predict(x_star)\n",
    "  return np.linalg.norm(u_pred - u_star, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(x_0, u_0, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)\n",
    "\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "g9aFDcGpxAq2",
    "lines_to_next_cell": 0,
    "outputId": "c6da5f52-af3c-4c1a-cd0c-a7221c33fc0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.543x288.159 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PINN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
