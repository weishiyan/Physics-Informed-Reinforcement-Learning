{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time \n",
    "from pyDOE import lhs\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMEGA = 1\n",
    "THETA_0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "\n",
    "\n",
    "# DeepNN topology (1-sized input [t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "list_of_layers = []\n",
    "\n",
    "max_node = 80  # List size\n",
    "num_list = 1  # number of layer list\n",
    "num_hidden = 8  # number of hidden layers\n",
    "interval = max_node/num_list\n",
    "\n",
    "for i in range(num_list):\n",
    "    temp_list = [1]\n",
    "    # Number of hidden layers\n",
    "    for j in range(num_hidden):\n",
    "        temp_list.append(max_node)\n",
    "    temp_list.append(1)\n",
    "    max_node -= interval\n",
    "    list_of_layers.append(temp_list)\n",
    "\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 5000\n",
    "lr = 0.001\n",
    "beta1=0.9\n",
    "eps = 0.1\n",
    "noise = 0.01\n",
    "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=lr,\n",
    "                                        beta_1=beta1,\n",
    "                                        epsilon=eps)\n",
    "\n",
    "# # Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "# nt_epochs = 2000\n",
    "# nt_config = Struct()\n",
    "# nt_config.learningRate = 0.8\n",
    "# nt_config.maxIter = nt_epochs\n",
    "# nt_config.nCorrection = 50\n",
    "# nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(N_u, N_f, noise=0.0):\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "#     lb = 0\n",
    "#     ub = num_pi *np.pi\n",
    "#     # Generating the t points for f with a N_f size\n",
    "#     # We pointwise add and multiply to spread the LHS over the domain\n",
    "#     X_f = lb + (ub - lb) * lhs(1, N_f)\n",
    "#     Exact_u = THETA_0*np.cos(OMEGA*X_f)\n",
    "#     X_u_train = lb + (ub - lb) * lhs(1, N_f)\n",
    "#     u_train = THETA_0*np.cos(OMEGA*X_u_train) + noise*np.random.randn(N_f, 1)\n",
    "    \n",
    "    npz = np.load(\"Deep_Deterministic_Policy_Gradients_notebooks/data/pendulum_data.npz\", allow_pickle=True)\n",
    "    # Loading data and ordering it to N x 1 array\n",
    "    theta = npz[\"theta\"][:,None]\n",
    "    times = npz[\"times\"][:,None]\n",
    "    \n",
    "#     # Trying something out with more dense x-axis\n",
    "#     list_size = theta.shape[0]\n",
    "#     steps = 0.01\n",
    "#     times = [x*steps for x in range(list_size)]\n",
    "#     times = np.array(times) \n",
    "#     times = times[:, None]\n",
    "    \n",
    "    # Adding noise\n",
    "    mu, sigma = 0, noise\n",
    "    # give same shape as data\n",
    "    random_noise = np.random.normal(mu, sigma, [len(theta)])\n",
    "    theta = theta + random_noise[:, None]\n",
    "    \n",
    "    # Reduce data set size for testing\n",
    "    theta_train = theta[:N_u]\n",
    "    time_train = times[:N_u]\n",
    "    \n",
    "    lb = times.min(axis=0)\n",
    "    ub = times.max(axis=0)\n",
    "#     time_train = lb + (ub - lb) * lhs(1, N_u)\n",
    "    \n",
    "    return times, theta, time_train, theta_train, lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, frequency=10):\n",
    "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __get_elapsed(self):\n",
    "        return datetime.fromtimestamp(time.time() -\n",
    "                                      self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "    def __get_error_u(self):\n",
    "        return self.error_fn()\n",
    "    \n",
    "    def log_time(self):\n",
    "        return time.time() - self.start_time\n",
    "#         return datetime.fromtimestamp(time.time() -\n",
    "#                                       self.start_time)\n",
    "    \n",
    "    def set_error_fn(self, error_fn):\n",
    "        self.error_fn = error_fn\n",
    "\n",
    "    def log_train_start(self, model):\n",
    "        print(\"\\nTraining started\")\n",
    "        print(\"================\")\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "        if epoch % self.frequency == 0:\n",
    "            print(\n",
    "                f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \"\n",
    "                + custom)\n",
    "\n",
    "    def log_train_opt(self, name):\n",
    "        # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "        print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "    def log_train_end(self, epoch, custom=\"\"):\n",
    "        print(\"==================\")\n",
    "        print(\n",
    "            f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \"\n",
    "            + custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_func(s):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "        # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "#         self.u_model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.nu = nu\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.t_f = tf.convert_to_tensor(X_f, dtype=self.dtype)\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_sum(tf.square(u - u_pred)) + tf.reduce_sum(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the input we’ll need later, t\n",
    "            tape.watch(self.t_f)\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(self.t_f)\n",
    "            # Deriving INSIDE the tape (since we’ll need the t derivative of this later, u_tt)\n",
    "            u_t = tape.gradient(tf.math.asin(u), self.t_f)\n",
    "\n",
    "        # Getting the other derivatives\n",
    "        u_tt = tape.gradient(u_t, self.t_f)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        # Buidling the PINNs\n",
    "        return u_tt\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "#             print(loss_value)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.u_model.trainable_variables))\n",
    "            self.logger.log_train_epoch(epoch, loss_value)\n",
    "            if epoch == tf_epochs - 1:\n",
    "                final_loss.append(loss_value.numpy())\n",
    "            if epoch % 10 == 0:\n",
    "                plt.clf()\n",
    "                plt.scatter(X_u, u, marker='.')\n",
    "                plt.scatter(X_u, self.u_model(X_u), marker='.')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Training and plotting the results'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Training and plotting the results\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the model and training\n",
    "# logger = Logger(frequency=10)\n",
    "# # x, t, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "# X_f, Exact_u, X_u_train, u_train, lb, ub = prep_data(N_u, N_f, noise=noise)\n",
    "# # plt.plot(Exact_u, \".\")\n",
    "# print(X_u_train.shape, u_train.shape)\n",
    "# plt.scatter(X_u_train, u_train, marker='.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4115d706675a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExact_u\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExact_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mfinal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0maverage_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-e596ece48dd4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, tf_epochs, nt_config)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m#             print(loss_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1955\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# delays. See b/136304694.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         with backend.name_scope(\n\u001b[0;32m--> 485\u001b[0;31m             scope_name), distribution.extended.colocate_vars_with(var):\n\u001b[0m\u001b[1;32m    486\u001b[0m           update_ops.extend(\n\u001b[1;32m    487\u001b[0m               distribution.extended.update(\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcolocate_vars_with\u001b[0;34m(self, colocate_with_variable)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;34m\"\"\"Does not require `self.scope`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     \u001b[0m_require_strategy_scope_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2122\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvariable_created_in_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcolocate_with\u001b[0;34m(op, ignore_existing)\u001b[0m\n\u001b[1;32m   5118\u001b[0m \u001b[0;31m# only API for those uses to avoid deprecation warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5120\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_colocate_with_for_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_existing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_colocate_with_for_gradient\u001b[0;34m(op, gradient_uid, ignore_existing)\u001b[0m\n\u001b[1;32m   5099\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minternal_convert_to_tensor_or_indexed_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5101\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(device_name_or_function)\u001b[0m\n\u001b[1;32m   5027\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0meager\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5028\u001b[0m   \"\"\"\n\u001b[0;32m-> 5029\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name_or_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5031\u001b[0m       raise RuntimeError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_loss = []\n",
    "\n",
    "# list of average loss for each layer\n",
    "layer_avg_loss = []\n",
    "layer_avg_time = []\n",
    "\n",
    "n_times = 3\n",
    "for layers in list_of_layers:\n",
    "    final_loss = []\n",
    "    average_time = []\n",
    "    for i in range(n_times):\n",
    "        # Creating the model and training\n",
    "        logger = Logger(frequency=10)\n",
    "        # x, t, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "        X_f, Exact_u, X_u_train, u_train, lb, ub = prep_data(N_u, N_f, noise=noise)\n",
    "\n",
    "        pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_u_train, ub, lb, nu=OMEGA**2)\n",
    "        def error():\n",
    "            u_pred, _ = pinn.predict(X_f)\n",
    "            return np.linalg.norm(Exact_u - u_pred, 2) / np.linalg.norm(Exact_u, 2)\n",
    "        logger.set_error_fn(error)\n",
    "        pinn.fit(X_u_train, u_train, tf_epochs)\n",
    "        final_time = round(logger.log_time(), 3)\n",
    "        average_time.append(final_time)\n",
    "\n",
    "        # Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "        u_pred, f_pred = pinn.predict(X_f)\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.scatter(X_u_train, u_train, label=\"input\", marker='.')\n",
    "        plt.scatter(X_f[:N_u], u_pred[:N_u], label=\"predicted\", marker='.')\n",
    "        plt.title(f\"Data vs Predicted\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        \n",
    "        plt.savefig(f\"./data/layers{layers}_{N_u}DataPoints_loss{str(round(final_loss[-1],3))}_time{final_time}_epoch{tf_epochs}_lr{lr}_beta{beta1}_eps{eps}_noise{noise}.jpg\", dpi=300)\n",
    "        # plt.show()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "    layer_avg_loss.append(np.average(final_loss))\n",
    "    layer_avg_time.append(np.average(average_time))\n",
    "    \n",
    "for i in range(len(list_of_layers)):\n",
    "    print(f\"For the {list_of_layers[i]} layer data: \")\n",
    "    print(f\"Average loss: {layer_avg_loss[i]}\")\n",
    "    print(f\"Average time: {layer_avg_time[i]}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_u_train, ub, lb, nu=OMEGA**2)\n",
    "# def error():\n",
    "#     u_pred, _ = pinn.predict(X_f)\n",
    "#     return np.linalg.norm(Exact_u - u_pred, 2) / np.linalg.norm(Exact_u, 2)\n",
    "# logger.set_error_fn(error)\n",
    "# pinn.fit(X_u_train, u_train, tf_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "# u_pred, f_pred = pinn.predict(X_f)\n",
    "# plt.scatter(X_u_train, u_train, label=\"input\", marker='.')\n",
    "# plt.scatter(X_f[:N_u], u_pred[:N_u], label=\"predicted\", marker='.')\n",
    "# plt.title(\"Data vs Predicted (4pi)\")\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.savefig(f\"./data/layers{layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "#   Exact_u, X, T, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "Rmd,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
